\input{boilerplate.tex}

\usepackage{hyperref,texlinks}

\begin{document}
 Алексей Николаевич Сальников\\
 email \href{mailto:alexey.salnikov@gmail.com}{alexey.salnikov@gmail.com}, \href{mailto:salnikov@cs.msu.ru}{salnikov@cs.msu.ru}\\
 VK \href{http://vk.com/asalnikov}{asalnikov}\\
 telegram \href{tg:???}{???}\\
 Skype \href{skype:salnikov\_alexey}{salnikov\_alexey}
 
\section{Организационные вопросы}
 \begin{enumerate}
  \i Параллельные вычисления.
  \i Распределённые вычисления.
 \end{enumerate}
 Письменный допуск: 10 вопросов/задач, в т. ч. написать простенький код на бумажке.
 
 \section{Параллелизм}
 Человек задумывает программу как последовательную. Но всёё параллельное выполняется ужасно долго, и уже много лет компы так не делают. Они пытаются самостоятельно параллелить и асинхронизировать всё, что можно.
 
 Зачем параллелить?
 \begin{enumerate}
  \i Большой объём работы.
  Что такое "большой"? Из сложности вычислений знаем об оценках сложности алгоритмов:
  \begin{enumerate}
   \i Временная сложность.
   \i Пространственная сложность.
     f(size(input))
  \end{enumerate}
 \end{enumerate}

 Короче, параллелить надо.
 
 Как оценить эффективность?
 
 График "расстояние между вычислениями"---"быстродействие".
 
 Уровни параллелизма:
 \begin{enumerate}
  \i Вентили. FPGA/ПЛИС (программируемые логические интегральные схемы, процессор собирается как конструктор под поставленную задачу, обеспечивая высокое быстродействие, параллелизм и др.) Процессор описывается как абстракция "схема из функциональных элементов с задержками". Впрочем, эти вентили медленнее, чем традиционные процессоры, без возможности программирования. Программируются схемы на VHDL и Verilog. В Таганроге придумали COLAMO — интерпретатор чего-то, похожего на Паскаль, в программу для ПЛИС, стараясь выполнить всё в один такт.
 
  \i Регистры, АЛУ, УУ, шины, прочее.
  У процессора есть система команд (то, что процессор может делать), они реализованы через микрокод (то есть захардкоженная в процессор последовательность элементарных действий, которые он выполняет при поступлении команды).
 
  \i Ядра в процессоре (многоядерность).
  Ядра могут лезть в чужие кэши, есть общий кэш. Ядра общаются по шине типа Hypertransport. 
 
  \i Процессоры в системе (многопроцессорность).
  Имеем в виду, что на одной плате.
 
  \i MPP (Multiprocessor)
  Плат может быть много, но все соединены, условно говоря, в один комп, управляемый одной ОС. Типичный пример --- мейнфрейм.
 
  \i Вычислительный кластер. 
  Время передачи данных между машинами уже велико. Нужно с этим считаться. Характеристики: надёжность и время передачи данных.
 
  \i Географически разделённый кластер.
  Сеть ненадёжна ("обычно данные доходят"). Скорость передачи данных не гарантируется (нужно, чтобы объём вычислений значительно превышал количество данных, которые нужны для них). Компьютеры кластера ненадёжны (можно заставлять несколько компов вычислять одно и то же, и Вы всё равно будете после этого в выигрыше (уменьшив риск того, что вычисление прервётся из-за выхода одного компа из строя)).
 
  \i GRID-система (сеть из кластеров).
  Сеть может падать, но сами элементы надёжны (кластер же отказоустойчив). Передача данных по сети слабо гарантируется, но вычисления очень объёмны.
 \end{enumerate}
  
 \section{ Классификация Флина по потокам команд и данных}
 УУ --- поток команд.
 
 АЛУ --- поток данных.
 
 S -- Single\\
 M -- Multiple\\ 
 I -- Instruction\\
 D -- Data
 
 SISD -- одно АЛУ, одно УУ, архитектура как по фон Нейману.\\
 SIMD -- берём одну инструкцию, но применяем её к нескольким регистрам (т. е. к множеству данных). [Векторные процессоры и векторные инструкции]\\
 MISD -- в жизни особо не встречается, к нему можно отнести предсказание переходов.\\
 MIMD -- почти всегда в жизни встречается именно оно.
 
 Характеристики процессора:
 \begin{enumerate}
  \i  Конвейер.
   TODO: скопировать из википедии, зачем
   Разгон конвейера: время, через которое быдет выдан результат, после того, как в него попадёт первая инструкция. Далее ответы будут выдаваться каждый шаг.
   Конвейеру мешают: 
    \begin{enumerate}
   	 \i промахи кэша (при переключении потока это будет почти наверное)
	 \i зависимости в данных
    \end{enumerate}
   С конвейером умеет обращаться компилятор, стараясь сделать получше. Флаги gcc: -march, -mtune.
  \i Суперскалярность.
  \i Предсказание переходов. Нужны теневые регистры (не видимые программному коду). Если есть ветвление, то все ветки выполняются одновременно (если могут), результат хранится в теневых регистрах, а когда выясняется результат условия, просто выдаётся нужный уже посчитанный результат.
   Хорошо работает на RISC-системах, ибо там фиксированный размер команд.
 \i Набор инструкций (векторные инструкции: стандарты MMX, SSE, AVX).
  Обычные регистры начинают считаться "сцепленными векторными регистрами".
 \i VLIW. Большая длинная команда, в которой кодируются отдельные маленькие команды, которые позволяют параллелиться на уровне процессора. Плюсы: компилятор может всё очень распараллелить.
  Минусы: это слишком сложно для компилятора. Часты полупустые VLIW, когда команд недостаточно. Код программы увеличивается в размере. Кэш-промахи чаще, ибо всё большое. Примеры: Itanium, Эльбрус.
 \i SMT (Simultaneous Multiple Threading) / Hyperthreading
  Поддерживаем два потока команд (для разных процессов). Если команды не конфликтуют по обращениям к АЛУ, например, то запускаем обе. Иначе одну. За счёт этого ускорение.
  Ныне дублируем регистры, прочее. Выигрыш идёт за счёт всяких маленьких ожиданий, типа обращений к кэшу. Ускорение может быть почти как при многоядерности.
  Безопасность: программа злоумышленника может видеть чужие регистры.
 \i Многоядерность.
 \end{enumerate}

 \section{Литература}
 \begin{enumerate}
  \i Архитектура.
   Документация от производителей.
   Стандарт Infiniband.
  \i Администрирование и анализ программ.
   Книги от специалистов:
    Лацис А. О. "Как построить и администрировать суперкомпьютер".
    С. А. Жульматий, Дацюк. "Администрирование суперкомпьютеров и кластерных систем".
   Учебники:
    В. В. Воеводин, Вл. В. Воеводин. "Параллельные вычисления" (см. про старые системы и про векторные машины).
    Воеводин В. В. "Вычислительная математика и структуры алгоритмов"
    К. Ю. Богачёв. "Основы параллельного программирования"
    Гергель В. П. (Декан нижегородского фивта) "Теория и практика параллельных вычислений".
    Jan Foster "Design and building \dots"
  \i Технологии программирования
    OpenMP Barbara Chapman "Using openMP"
    MPI William Gropp "Using MPI": части 1, 2, 3.
 \end{enumerate}
 
 Конвейер: m стадий, $t_i$ работает каждая стадия, массив данцемула вычисления времени срабатывания конвейера.
 Конвейер с зацеплением: параллелит какие-то части, например, часть работы отдаёт сумматору, часть - умножителю, они работают вместе, а потом в общих стадиях комбинируются в результат.
 
 Лёгкие ядра.
 Рядом с обычным ядром и его обвязкой располагается множество мелких ядер(SPU), которые умеют поменьше, сами попроще, но их много. Пример: IBM Cell.
 Так устроены графические процессоры: общие большие регистры, общая большая память, множество мелких ядер, образующих трёхмерную решётку (3D Mesh). К решётке можно добавить ядер, соединив видеокарточки по SLI.
 Для фрагментов решётки определённого размера (трёхмерного кубика) своё УУ. Такты считаются для каждого кубика отдельно, и все элементы кубика за такт занимаются одной и той же работой (например, перемножают параллельно одну и ту же матрицу).
 \wikilangref{ru}{ATI (AMD) OpenCL}{OpenCL} переносим, будет эмулироваться на процессоре, если подходящего графического устройства не нашлось, но умеет мало.
 \wikilangref{ru}{CUDA}{NVIDIA CUDA}. Говорят, программировать приятнее. 
 
 Классификация многопроцессорных компьютеров по доступу к памяти
 \begin{enumerate}
  \i Адресное пространство
  \i Скорость обращения к ячейке.
 \end{enumerate}
 
 \begin{enumerate}
  \i \wikilangref{ru}{\detokenize{Симметричная мультипроцессорность}}{SMP (Symmetrical Multiprocessing)}/\wikilangref{ru}{Uniform Memory Access}{UMA (Uniform Memory Access)}
   Адресное пространство общее. Обращение из любого процессора к любой ячейке памяти за одно и то же время.
   Это --- шинная архитектура. Банки памяти и процессоры висят на одной шине. Дорого делать, если процессоров больше 2. И если процессоров много, будут конфликты за шину, и не будет значительного прироста производительности.
  \i \wikilangref{ru}{\detokenize{Non-Uniform_Memory_Access}}{NUMA(Non Uniform Memory Access)}
   Обращение к разным ячейкам памяти за разное время. У каждого процессора появляется ``своя'' память, к которой обращения быстрее всего. 
   Это --- процессоры со своей памятью, соединённые своей шиной, провязанные все общей шиной.
  \i ccNUMA (Cache Coherent NUMA). Записи в память производятся на самом деле в кэш. Кэш - отдельная многопроцессорная система, которая сама синхронизируется, когда потребуется, и обеспечивает желаемое поведение.
   Синхронизация кэшей - аппаратный протокол, и его реализовывать - дорого, если процессоров много.
  \i MPP (Massive Parallel Processing) Отказыеваемся от общего адресного пространства. Отдельный шкафчик с платами.
   Называем кластером, если не один шкафчик. Каждая машина --- узел кластера. 
   На всём этом должна стоять одна ОС, и работать это должно как единое целое.
 \end{enumerate}
 
 \section{Пример. Вычислительный центр.}
 Контора, в которой стоят компы (т. е. вычислительные системы). Обращение к нему пользователи ведут через Интернет. Есть сайтик, почтовый сервер (рассылает уведомления о проблемах на вычислительных системах). Сайтик --- высокоуровневый интерфейс, в котором выдают логины и просят у пользователей годовые отчёты об их деятельности. 
 
 Устройство:
 \begin{enumerate}
  \i Интерфейсная машина. На неё заходят, компилируют, редактируют код, делают всякие вещи. Запускать на ней вычисления не следует, на это админы ругаются.
  \i Контрольный сервер (один или много). Если всё не так, то зажигается красная лампочка. Или совсем красная лампочка, если всё ещё хуже. Он может сказать, какая именно машина вышла из строя, чтобы можно было её заменить. Возможно, всё это происходит со свистелками.
  \i Сервер очередей. Иногда распределение задач занимает отдельный сервер.
  \i Узлы кластера, на которых всё считается.
  \i Хранилище (storage). Тоже кластер, процессоры слабые, но большие жёсткие диски.
 \end{enumerate}

 Сети, которые всё это соединяют:
 \begin{enumerate}
  \i Управляющая сеть.
  Обычная компьютерная сеть, например, Ethernet. Предназначена для управления и подгрузки кода, который будет выполняться.
  \i Сеть синхронизации. Для синхронизации часов на различных процессоров (точнее, чем NTP, который с точностью до секунд). Он не просто скачком переводит время, а ускоряет часы машин, которые отстают, и замедляет, когда машина спешит. Реализуется в виде коаксиального кабеля, по которому идут импульсы точного времени.
  \i Сеть ввода-вывода. Может быть объединена с вычислительной сетью.
   Некоторые её узлы связаны с хранилищем, другим узлам данные передаются через другие узлы как через прокси.
  \i Сеть взаимодействий типа "точка-точка". Если какие-то два узла кластера хотят общаться между собой быстро и много, их соединяют так.
  \i Сеть коллективных операций. Если надо что-то распространять по узлам кластера почти так же, как в точка-точка.
\end{enumerate}

% 04.10.2017
\section{Софт для обеспечения работы кластера}
Задачи:
\begin{enumerate}
 \i Мониторинг живучести узлов и оборудования.
 ganglia собирает информацию от отдельных узлов  в одном месте, nagious анализирует её и информирует, если что-то не так. Работает для небольших кластеров.
 Если кластер большой, то встаёт задача анализа данных, поиска аномалий по куче логов. 
 \i Запуск. Если кластер большой, то одновременное включение всех узлов вызовет просадку напряжения в сети. Поэтому все узлы включаются постепенно (сигналы по протоколу SMPI или IPMI).
  Перед этим включается система охлаждения (это важно). 
  Узел получает IP-адрес обычно по протоколу dhcp. Если это будет происходить очень быстро, будет большая нагрузка на сеть, пакеты с IP-адресами могут потеряться, и узлы не получат ссебе адрес. Поэтому нужно модифицировать DHCP-протокол. Можно сделать сеть управляющих компов звёздчатой структуры (но тогда будет нагрузка на промежуточные компы, это нужно учесть).
 \i Где взять образ ОС?
   Дисковая система: базовый образ рассылается по протоколу tftp почти так же, как в предыдущем пункте. Они поднимают NFS(Network File System). NFS имеет только один сервер, поэтому пользовательские данные там не хранятся. 
   Пользовательские данные хранятся в параллельной файловой системе. Они пытаются распределять данные так, чтобы они были близко к обрабатывающему их узлу, следят за надёжностью и обеспечивают POSIX-интерфейс обращений к ней. Примеры: lustre и gpfs
  \i Авторизация пользователей.
   Как выяснять информацию о пользователях с узла кластера?
   Можно на каждом узле хранить passwd, shadow, group, gshadow и синхронизировать их rsync. Почти нет нагрузки на сеть (синхронизация же редко).
   LDAP -- иерархическая база данных, в которой можно хранить данные о пользователях. Active Direcctory в Windows --- реализация LDAP-протокола. Большая нагрузка на сеть, если частые обращения. Могут даже замедлять работу кластера.
  \i Средства разработки. В основном на кластерах висит какая-то реализация MPI.
  Кросс-компиляция: у кластера есть разные виды машин (интерфейсная, вычислительный узел, ...). Поэтому используется компилятор, который компилирует программу не обязательно для архитектуры машины, на которой он стоит. Он использует чужой набор команд, а линковщик использует чужие библиотеки. Исполняемый файл отправляется на другую машину для исполнения. 
  \i И т. д. 
\end{enumerate}

Совет: дебаг можно проводить на модели кластера, например, из двух таких же машин, как там. Она позволит отловить совсем наивные ошибки. При отправке же на кластер можно очень долго стоять в очереди, чтобы программа потом упала за 4 минуты работы.

Управление очередями (Batch sheduling system)
 PBS --- Portable Batch System
 В 70-е использовали файл паспорта задачи, простой текстовый файл, который описывал множество ресурсов, которые должна потребить программа, которая будет выполнена. 
 PBS использует shell-скрипт, в котором специального вида (\# pbs ...) комментарии описывают ресурсы, которые будут использованы: 
 \begin{enumerate}
  \i число процессоров/узлов
  \i время (но тут нужно быть аккуратным: убивать задачи, которые превысили время, надо аккуратно, ибо их хозяин может рассердиться)
  \i память (оперативная, дисковая обычно не смотрится), лицензии на коммерческое ПО (например, fluid; кластеры для коммерческих нужд (не научных/образовательных) обычно используют много проприетарного ПО; система управления лицензиями flexlm)
 \i Оборудование (например, узлы с графическими ускорителями)
 \i Имя узла (можно попросить исполняться на конкретном узле; исполняться задача обычно начинает реже)
 \end{enumerate}
 Популярные планировщики: openpbs, Moab, slurm (slurm-lnl в Debian), LoadLeveler, Sun Grid Engine (с продвинутой системой распознавания пользователя с сертификатами).
 
 slurm
 
 Интерфейсная машина, демон-контролёр (на своей машине), демон-пускач. Общение пользователя происходит через клиентское приложение с контролёром. Он знает текущее состояние кластера и очередь задач. Он решает, когда что выполнять. Если саму очередь поддерживать сложно, выделяется отдельный демон ведения очереди (планировщик заданий). Когда контролёр решает поставить задачу в очередь, он говорит демонам-пускачам, которые, работая как init, создают нужное дерево процессов. Когда выполнение закончится, он же и завершает процессы. Он же следит за здоровьем кластера с помощью вышеупомянутых ganglia и nagious.
 
 В очередь можно поставить только shell-скрипт. Делается это с помощью sbatch. Параметры к нему могут быть такими же, как паспорт задачи. Можно указать файлы ввода-вывода. Интерактивности не предполагается.
 \begin{enumerate}
 	\item srun делает то же самое, только блокирует консоль до завершения.
 \end{enumerate}
 
 set -a печатает все переменные окружения из скрипта запуска. В том числе job id. Можно будет смотреть информацию (например, о доступных ресурсах, о занятости кластера, ...) командой sinfo, 
 Смотреть очередь можно командой squeue.
 scancel убивает задачу.
 scontrol позволяет интерактивно просматривать и настраивать задачу через общение с сервером-контролёром. Например, можно запретить принимать новые задания в течении 3-х дней. Используется сисадминами.
 smap визуально рисует загруженность узлов.
 
 slurmctld --- главный демон.
 ??? --- демон-пускач.
 slurmbdb управляет отправкой событий в базу данных при операциях с задачами. Восстанавливает после, например, перебоев с электроснабжением.
 
 \section{Алгоритмы планирования}
 \begin{enumerate}
 \i FIFO. Единственная честная. Низкая загрузка кластера зачастую. Несправедливость к маленьким задачам (они будут ждать так же, как большие).
 \i FIFO с приоритетами.
 \i Gang-алгоритм. Делим задачи в завичимости от потребляемых ресурсов в несколько FIFO с приоритетами.
 \i Backfill. Вычислительная система --- стакан, дно --- многомерная поверхность, на которой указано множество ресурсов. Вертикально растёт время. Задача --- сыграть в тетрис.
  Окно - промежуток времени и множества ресурсов, которое может быть захвачено процессом. Приходящий процесс пытается занять ближайшее по началу выполнения окно, в которое он помещается. В случае конфликтов в ранние окна ставятся задачи, у которых приоритет выше.
 \end{enumerate}

%11.10.17
\section{Система приоритетов задач пользователя}
Как сделать так, чтобы никого не обделить?
Топорков В. В. --- человек, занимающиеся алгоритмами планирования задач, в первую очередь планированием задач для GRID.
Проблемы с честностью:
\begin{enumerate}
	\i Могут быть срочные задачи относительно фона.
	\i "Переключение" на более срочную задачу. Сложность в хранении состояния оперативной памяти, промежуточных вычислений. Это очень много. Поэтому задачу просто убивают, но в очереди задача остаётся на первом месте, чтобы начать выполнение сначала.
	Поддержка контрольных точек: в какие-то моменты, определяемые событием в коде или просто астрономическим временем, программа по своей инициативе сбрасывает своё промежуточное состояние в файловую систему. Есть библиотеки, автоматически расставляющие контрольные точки, но они не очень хороши: программист лучше знает, когда вставить точку.
	\i Замечание: если задача хотела себе много ресурсов, а пока она стояла в очереди, ресурсы пропали (кластер отвалился), то она не начинает выполняться, когда подходит её очередь, а остаётся ждать в начале очереди, пока ресурсы вернут. Считается, что задача когда-то начнёт выполняться. Так что это не проблема с честностью.
	\i Приоритеты у пользователей (id), групп (gid), проект (account в терминах slurm)
	\i Partition -- кусок кластера. Можно отдать его, например, группе пользователей, и если они будут ставить задачи в этот раздел, то они получают наибольший приоритет (остальных пользователей оттуда выгоняют). Популярно, если нужно что-то быстро посчитать, но не отдавая весь кластер. Минусы: мощности меньше, чем у всего кластера.
	\i Некорректное использование вычислительной системы.
	Пример: на кластере 21к процессоров, одному пользователю надо 20к процессоров на 10 минут, другому -- 1.5к на 2 недели. Первый может оказаться в пролёте.
	Надо стимулировать пользователей не ставить такие задачи, как второй. На каждого пользователя заводится счётчик суммарных процессор-часов. И есть разные отметки количества (т. е. пользователей "раскидывают по корзинам"). Пользователииз корзин с меньшим временем имеют больший приоритет, чем все из больших корзин. Счётчик сбрасывается когда админ захочет, например, раз в месяц. Такой подход используется в РАН.
	Второй подход: на каждого человека отводится какое-то количество процессор-часов. Применяется в коммерческих вычислительных центрах.
	
	"Рыночные" алгоритмы планирования.
	Каждому пользователю выдаются фишки приоритета, которые он раскладывает на свои задачи. Устраивается аукцион среди всех задач с такими фишками. 
	
	\section{Интерконнект в кластере}
	Отличия от сети:
	Предсказуемость (считается, что узлы чаще всего доступны). Топология не меняется во время жизни кластера (не факт).
	Ещё что-то.
	
	Характеристики:
	\begin{enumerate}
		\i Латентность (накладные расходы на передачу данных, сек). Неотделмио от самих данных, поэтому меряется в процессе передачи данных. Это задержка между началом передачи и окончанием приёма. Латентность растёт вместе с объёмом данных (накладные расходы тоже растут, и поэтому отделить объём данных от латентности не получится). 
		\i Скорость передачи данных (пропускная способность, байт/сек).
			Пример: грузовик с винчестерами обеспечивает охрененную пропускную способность, но очень высокую латентность.
		\i Темп выдачи сообщений. Важна для посылки коротких сообщений. Время, которое затрачивается перед повторной отправкой сообщения ("насколько часто Вы можете флудить в сеть").
		\i Устойчивость. "Как данные доставляются в среднем". Можно сказать, что это дисперсия времени передачи одинаковых сообщений между двумя фиксированными узлами в фиксированном направлении. Если интерконнект, дисперсия мала по сравнению со временем передачи. Если через Интернет, до дисперсия может многократно превышать время передачи. 
		При отправке больших порций данных влияние этого отклонения нивелируется, но для малых (и частых) сообщений она очень значима.
	\end{enumerate}
\end{enumerate}

 \section{Способы соединения узлов в кластере}
 Способ соединения узлов в кластере называется топологией (т. е. устройство графа соединений). Топология выбирается при проектировании СК исходя из его задач.
 Вершины графа -- узлы, рёбра -- линки. "Вычисляющая сущность" является узлом (кружок). Узлом может являться "свич" -- устройство, занимающееся только передачей через себя данных (квадратик). Эти две роли можно и совмещать (выпуклый многоугольник).
 
 Классические топологии:
 \begin{enumerate}
 	\i Звезда. Один свитч и несколько компов. Так обычно устраивают компьютерные классы. Все компы висят на одном проводе. В реальности провод закольцован, и он оптоволокно.
 	\i Линия. У каждого узла 2 сетевых вывода. Соединяется по цепочке.
 	Применяется редко, для небольших вычислительных кластеров и в основном для конвеерной обработки данных: когда можно подсчитать кусочек, передать на следующую машину, а самому заняться задачей, аналогичной исходной.
 	Применяется военными, ибо у них часто есть много стадий предобработки данных.
 	\i Кольцо. Зацикливаем линию. Могут быть проблемы со скоростью передачи данных (по витой паре, по оптоволокну не очень много) между первым и последним элементом кольца, если они далеко друг от друга физически.
 	Часто применяется для соединения территориально удалённых вещей оптоволокном (довольно дёшево, бовольно быстро).
 	\i Решётка. 
 	Популярна, ибо соответствует структуре задач, появляющихся естественным образом во всяких урматах и вычислительной физике (если часто идут обращения лишь к соседям). Например, рассчитать прочность моста, промоделировать климат, рассчитать обтекание чего-нибудь газом или жидкостью.
 	Например, максимальный транзит $2n-2$.
 	\i Тор. Решётка, свёрнутая бубликом.
 	\i Гиперкуб. Ранга 1 -- отрезок, ранга 2 -- квадрат, ранга 3 -- куб, ранга 4 -- тессеракт. Напоминает решётку, но разница в том, что тут действительно лишь один кубик.
 	Максимальный транзит -- ранг гиперкуба, рост количества вершин и рёбер экспоненциален.
 	Используется, когда очень важно минимизировать транзит, за счёт стоимости.
 	\i Деревья. 
 	Для сбора информации в один приёмник, например.
 	\i Смешанные. Например, если у решётки выясняется, что время передачи до удалённых вершин велико, можно пробросить провода каждые 2 узла.
 	Например, 6D-тор. Это тор с дополнительными линиями (который оказывается шестимерным тором). Это не означает, что у задачи шестимерная топология, это просто пробросили доп. рёбра, чтобы уменьшить транзит.
 	Можно, например, сделать решётку из гиперкубов (быстрая связь внутри групп, медленная между группами). На этом построена топология Dragonfly (там трёхуровневая вложенность, но нечто попроще, чем гиперкубы). 
 	Топология Butterfly. Если линков много, то это называется жирным деревом.
 \end{enumerate}

 Характеристики, влияющие на латентность:
 \begin{enumerate}
 	\i Максимальный транзит (диаметр графа) -- расстояние между двумя наиболее удалёнными вершинами. 
 	Сумма переходов (транзитов) как функция от числа узлов.
 	\i Средний транзит.
 	\i Надёжность (сколько рёбер нужно удалить, чтобы граф стал несвязным).
 	\i Стоимость (отношение числа рёбер к числу вершин, прочее).
 	\i Степень вершины.
 	\i Величина бисекции (минимальное количество рёбер, которое нужно удалить, чтобы граф разбился на две части с одинаковым числом вершин). Больше бисекция -- выше стоимость топологии, но лучше для передачи данных.
 	Это не то же самое, что надёжность. Надёжность -- о потере лишь одного узла. Бисекция -- о разбиении графа пополам, с надёжностью это связано мало, скорее это о передаче данных.
 	\i Масштабируемость. Сколько рёбер нужно добавить, чтобы добавить ещё хотя бы один узел с сохранением топологии сети. Например, чтобы добавить узел решётки, нужно добавить целый столбец, и рёбра ко всем ним. Хуже всего с гиперкубом, ибо нужно построить ещё один гиперкуб и провести много рёбер. 
 \end{enumerate} 

 \section{Пример. Телефонные сети.}
  Как были устроены телефонные сети в прошлом (когда уже было много телефонных станций, а не только одна на город).
  Вы звоните на телефонную станцию барышне. Просите соединить с кем-то. Если он на другой телефонной станции, то ждём освобождения канала между станциями. После барышня вручную вставляет Ваш кабель в разъём канала, с другой стороны делают то же самое, но для принимающей стороны. Это называется "свитч с установлением соединения". Проблема: на время установления соединения (барышень ведь конечное число) и передачи данных (каналов ведь не бесконечное число) никто больше пользоваться не может. Решение: делим всё на пакеты, и передаём по кускам. Проблема: частые переключения заставляют свитч ощутимо греться.
  Свитч теперь обладает оперативной памятью, в которой хранит пакеты перед отправкой. Постоянно подключены несколько кабелей, и со всеми можно общаться одновременно.
  Современные свитчи есть комбинация: там есть и аппаратная часть, отвечающая за переключение линков, и программная, отвечающая за пакеты.
  
  \section{Организация маршрутизации}
  Статическая.
  Динамическая. В каждый конкретный момент времени существует какой-то маршрут. Как именно пойдёт сообщение определяется самой сетью. Например, она может учитывать загрузку отдельных узлов, географическое расположение узлов, прочее. Гарантируется время доставки в среднем. Устойчивость будет плохой.
  Обычно маршруты составляются заранее, а при необходимости передачи данных берётся первый маршрут, если где-то по длине маршрута свитч занят, берётся следующий маршрут. Если все заняты, то ждём. Конкретная организация интерконнекта обычно запатентована создавшей её фирмой.
  
  \section{Infiniband}
  Для канала связи вычисляется его пропускная способность и сохраняется в виде какого-то числа фишек. Фишки свои для каждого линка из каждого узла.
  У каждого устройства есть GID (Gloabl Identificator) и LID(Local Identificator). Сетью управляет отдельно выделенное устройство Subnet Manager. Когда появляется новое устройство, Subnet Manager выдаёт ему LID, пересчитывает все таблицы пропускной способности (меряет пропускные способности проводов, то есть), которые нужно пересчитать, пересчитывает таблицы маршрутизации и рассылает их тем, кому это нужно знать. 
  Сеть Infiniband -- сеть Петри (сущность из дискретной математики). На каждом переходе пороговое значение. Сообщение длины n стоит какое-то количество фишек. Переход срабатывает, если фишек скопилось достаточное количество. При этом от него отнимается стоимость. Если все фишки израсходованы, передачи не происходит.
  
  Если выясняется, что узел долго не отвечал на запросы, он помечается как умерший и удаляется из таблиц маршрутизации тех, кто с ним общался (Subnet Manager перестраивает их). 
  Читать спецификацию Infiniband, презентации от Mellanox.
  
 \section{Место для последней лекции}
 
 
 \section{Распределённые файловые системы для больших данных}
 Для того, чтобы обрабатывать большие данные, их нужно где-то хранить. Вертикальное масштабирование (scale up): покупаем всё бо'льшие диски. Горизонтальное масштабирование (scale out): берём несколько серверов и объединяем в систему. Об этом и пойдёт речь в курсе.
 
 Родоначальник -- GFS (Google File System), где люди из Google объясняют, как они реализовали на имеющемся железе удовлетворяющую их нуждам для Больших Данных (просто распределённые ФС уже были, а вот для обработки Больших Данных не было).
 
 Специфика BigData:
 Большие файлы ($>>1$ GB) (гугл скачивал себе весь Интернет)
 Commodity hardware (обычные сервера, которые производятся и покупаются в больших количествах, и надёжностью особой не отличаются).
 "Write once, read many"
  Робот скачивает интернет, потом его обрабатывают, в основном - последовательно, в основном-чтения.
  \begin{enumerate}
  	\item в основном --- последовательные чтения
  	\i ещё 3 пункта
  	\i не удалямем данные, а записываем новые (то есть, в серединку файла нельзя записать что-нибудь, придётся создать новый файл и удалить старый).
  \end{enumerate}

	Архитектура:
Master server, хранящий список файлов и их метаданные
Сами файлы разбиты на блоки(чанки).
Блоки хранятся на Chunk servers: обычные сервера (commodity hardware) с обычной Linux FS. А Master Server собирает это всё в единую ФС, раздаёт chunk-серверам команды, следит за работоспособностью. Запросы приходят на Master-сервер, а данные отдаются напрямую с chunk-серверов (иначе master-сервер будет бутылочным горлышком).

GFS был в виде статьи. Yahoo на тот момент написал HDFS (Hadoop Distributed File System) на Java и выкатил в OpenSource.
Master server == Name node. Chunk server == Data node. chunk == block

TODO: скопипастить со слайда Роли компонент.
в памяти (для быстрого доступа)
heartbeat идёт раз в 3-5 секунд
список блоков приходит каждые полчаса

Размер блока
В обычных ФС обычно 4 Кб. 
Зачем вообще разбивать на блоки? Почти любой файл влезет на большой диск. 
Если блок очень большой,
\begin{enumerate}
	\i Чем больше блок -- тем больше фрагментация.
	\i Хуже параллелизм.
\end{enumerate}
Если блок очень маленький, то: 
\begin{enumerate}
	\i Namenode хочет хранить всё в памяти. Каждый блок. Не влезет, и операции будут не очень быстрыми (если и влезет). 
	\i С каждым блоком хранится чексумма, и она создаёт накладные расходы. 
	\i Чаще всего читаем последовательно. Последовательное чтение быстро. Если блоки слишком маленькие, мы этим преимуществом не воспользуемся. Надо, чтобы время seek'а было мало по сравнению со временем последовательного чтения
	\i Обращение к Datanode тоже занимает время. 
\end{enumerate}

Проблема: очень маленькие файлы (меньше размера блока).
Физического ограничения, что меньше размера блока нельзя, нет. Поэтому мелкий файл будет занимать столько места на диске, сколько он и есть (+метаданные), но будет занимать отдельный блок на Namenode. Если таких мелких файлов очень много:
\begin{enumerate}
	\i трата оперативной памяти Namenode и повышенная нагрузка на него
	\i Хуже скорость чтения
\end{enumerate}
Вывод: лучше склеивать мелкие файлы.

Репликация:
Считаем, что места у нас много, поэтому можем не применять хитрых алгоритмов, а просто дублировать информацию.
Фактор репликации -- число копий каждого блока. Обычно 3. Копии распределяются по нодам с учётом топологии сети.
 \href{https://youtu.be/-hE8jkyfqMw?t=2m55s}
 Например, плохо дублировать файл на серверах одной и той же стойки, ибо у всей стойки может сломаться маршрутизатор. 
 
Чтение из HDFS.
TODO: Вставить картинку
Namenode ещё проверяет права доступа, наличие такого файла, папки, и проч.

Запись в HDFS
Вставить картинку

Клиент передаёт данные на одну Datanode, на которую ему сказал писать Namenode. Потом datanode'ы между собой разбираются, как делать репликацию. После того, как репликация будет произведена, клиенту приходит подтверждение. 

Если процессе записи datanode падает, ack пакет не приходит вовремя, клиентская библиотека сообщает об этом Namenode, Namenode выдаёт новый идентификатор блока (чтобы старый блок, если он частично записался, был помечен как мусор и удалён), и запись начинается снова.
Если, например, данные были записаны на 2 ноды, а третья упала, Namenode даёт приказ продублировать с какой-то из уже записанных нод, а клиенту уже говорят, что запись завершена. 
Заметим: общение между нодами происходит внутри кластера, где соединение довольно быстрое.

Недостатки:
\begin{enumerate}
	\i нет random reads/writes
	\i не подходит для low-latency приложений (как key-value хранилище, для систем реального времени, для случаев, когда пользователю требуется быстрый ответ (например, открыть документ))
	\i Проблема мелких файлов (картинки хранить не получится)
	\i Namenode -- единая точка отказа
	Поэтому Namenode покупается дорогим и отказоустойчивым, питание, сеть и прочее дублируются, а сама она окружается теплом и заботой. Но лучше резервировать.
\end{enumerate}

Secondary Namenode 
Сервер, хранящий реплику какого-то состояния Namenode.
Namenode помимо непосредственных изменений памяти пишет лог, и периодически пишет слепки памяти (например, можно выключить её с сохранением состояния и включить потом). Но передавать слепки памяти накладно, поэтому при помощи какой-нибудь системы ротации логи пачками пересылаются на Secondary Namenode, где поддерживается такое же состояние (с точностью до последнего пакета). Secondary Namenode пишет свои слепки памяти периодически, и может их отправлять на Namenode, чтобы следующий старнт был с более актуального состояния (и заодно Namenode не надо этим заниматься).

Если Namenode упала надолго, вручную вводится в строй Secondary Namenode как горячая замена (слегка отстающая, впрочем).
Если primary и secondary стоят физически рядом, можно просто подключать диск по NFS, чтобы автоматически все действия primary дублировались на secondary.
Нужно заметить, что эта система не очень приспособлена именно для горячей замены. Для этого есть другие.
Например, федерация Namenode: разбиваем на несколько Namenode, каждая из которых обслуживает свой раздел (напр., /home, /data). Вообще, в данном курсе такое не рассматривается.

Клиенты HDFS
Скопировать
Замечание: -cp работает через клиента, т. е. медленно. hadoop distcp производит копирование внутри кластера (или даже одной ноды). 

Лайфхак: можно так составить конфигурационный файл, чтобы Hadoop считал Ваш компьютер удалённым сервером. Полезно для отладки.


\section{семчик}
HDFS 2 PB -- имеем столько места на дисках
Block size 128 mb
avg metadata block 600 B -- метаданные (хранятся только на Namenode) на блок (включая метаданные на реплики)
replication factor=3

Namenode RAM - ?

2 PB/(3*128 MB)*600 B = 3 Гб



Кластер состоит из HDD-дисков. 
R = 130 МБ/сек
seektime = 1 мс
Размер блока - ?, чтобы время на seek составляло 0.1\% времени чтения.

seektime/ 0.0001 = 1000 мс = 1 с.
Размер блока >= 130 МБ.

\section{MapReduce}
Были действия,которые совершают над данными. Для каждого конкретного случая писали свой велосипед. Google же заметил сходство и написал универсальную реализацию.

\begin{enumarate}
 \i map
  $f: T \rightarrow [S]=[(k, v)]$.
  Функция применяется поэлементно к входным данным и может выполняться параллельно.

 \i shuffle \& sort
 \i reduce
  $g:k, [v] \rightarrow [(k', v')]$.
\end{enumerate}

Пример:
Лог веб-сервера. Хотим посчитать количество уникальных посетителей, сколько раз каждый пользователь посетил сайт. Различать пользователей можно по uid, который есть в логе. 
\begin{enumerate}
 \i map: выделяем из строки лога uid.
  $f: line->(uid, NULL)$
 \i shuffle\&sort
  Сортируем по uid. 
 \i reduce В сортированном списке количество уникальных элементов считается просто: количество смен ключа при последовательном чтении.
 $g: uid, []_n \rightarrow uid, n$
 $g: uniq -c$ --  утилита unix.
 
\end{enumerate}
cat access.log | ./get_uid | sort | uniq.c

Сравнение MPI и MapReduce.
MPI: 
Общее хранилище на SAN(Storage Area Network): данные хранятся отдельно, обрабатываются отдельно, любой узел имеет доступ к данным в равной мере
CPU-intensive (дефицитный ресурс --- процессор).


Минусы MPI.

Плюс: Задачи решаются за прогнозируемое время.


...
Вместо простой разбивки на блоки фиксированного размера вводится разбивка на сплиты: блоки, учитывающие внутреннее строение файла (например, для текстового файла -- по строкам). Поэтому сплит может быть больше по размеру, чем блок.

При отказе оборудования запускаем ту же задачу на другом сервере. Желательно, на том, где данные уже есть. 

\section{14.11.17}
grep на стадии map ищем, всё.

group by
map ничего, sort сортирует по ключу, reduce просто собирает куски с одинаковым ключом в группы.

reduce=sum
Выдаём промежуточно сумму.
reduce=avg
Выдаём сумму и количество. Тогда можно восстановить правильное среднее.
reduce=median
можем сжать повторяющиеся значения в пары (количество, ключ)
Но комбайн плохо всё равно работает.


Обратный индекс
В мапе к слову приписываем количество его встреч, свели задачу к предыдущей.

Семинар

path#name позволяет переименовать файл при загрузке

\section{Павел Ахтямов}
Строим частотный словарь слов Википедии. article -> <word> -> (word, 1) -(sort)> (word, <1,...,1>) -add> (word, count) 

Заметка: число мапперов нельзя указать, ибо зависит от разбиения на блоки. Это можно регулировать только глобальными параметрами.

Файл должен быть исполняемым. Для скриптов надо использовать \wikiref{shebang}{Шебанг (Unix)}.

regex101.com

Счётчики в MapReduce. reporter:counter:group,name,number


\section{Леонов, Отказоустойчивые распределённые системы}
wait-free
Независимо от того, какие сообщения приходят тебе и доходят ли твои сообщения до других, ты за константное время совершаешь прогресс.

Асинхронная SM. Барьер - примитив синхронизации.

1. Пусть два синхронизатора с разницей 2. Тогда 

2. Пусть получили сообщение от процессора в такте x-1. Но мы могли перейти в такт x только получив все сообщения от провессоров в такте x-1.

Разрешаем f отказов. Потоки отправляют свои значения. Когда получит n-f сообщений, выбирает f из n-f чисел детерминированной функцией, например, минимальных. И среди них хотя бы одно будет общим.

Семинар
Hive. Люди из Facebook заметили, что типичные задачи работы с хранилищем удобно представлять в чём-то, похожем на реляционную алгебру.

Над кластером строится Hive metastore, в котором хранится информация о таблицах: из каких файлов она состоит, а где они хранятся знает HDFS.
Metastore: table $\rightarrow$ location (папка HDFS) + информация о таблице (колонки, их типы, формат хранения).
То есть, это аналог Namenode. Само оно устроено в виде обычной БД (SQLite, \dots).

Driver Hive: HiveQL $\rightarrow$ MapReduce Jobs.
Преобразуем команду, похожую на обычный SQL-запрос, в каскад MapReduce'ов.


\section{Отказоустойчивые распределённые системы}
Руководство компании принимает спонтанные решения.
Алексей один, ему некому вредить. Поэтому и не смешно.
SSM -- та модель, которую используют при написании многопоточных программ. Точки синхронизации---барьеры--- создают общие для всех потоков часы.
SMP --- распределённая база данных с синхронизатором. Синхронизатор берёт всё асинхронное на себя, а остаётся синхронная часть.

Консенсус (задача о византийских генералах). Есть процессы и числа. Из всех предложенных чисел нужно выбрать одно. В ситуации с отказом число должно быть предложено не отказавшим процессом. 

GS == Global State

Сортировать надо не лексикографически(контрпример 01 10). Говорят, что достаточно выписать n состояний $0\dots0, 10\dots0, 110\dots0, 1\dots1$, которые из $I_1$  приводят к $I_2$. 


Terminated  .... Есть распределённая БД, надо, чтобы все узлы, реализующие её, получили значение, которое нужно записать

Есть цепочка банков, переводим с одного счёта на другой через всю цепочку.

CP -- не реагируем на записи, всегда возвращаем начальное состояние. 
AP -- Будем держать локальную базу данных и не общаться. Каждый поток возвращает своё.
CA -- Выделяем сервер, и обращаемся к нему как клиенты.

проксировать == переслать


\section{Сем}
ZooKeeper
\begin{enumerate}
	\i PDF
	\i Официальная документация
\end{enumerate}

Основная задача --- средство синхронизации (хотя можно хранить в нём что-то как в ДБ).

Chubby -- система Google для синхронизации хостов. 

Модель данных: древовидная.
Узел может делать get, get_data, get_children, set_data, create, remove, sync.

setData(str, version) -- аналогично CAS.

\section{Прикладная задача}
Есть пользователь, который хочет купить что-то в интернет-магазине. Его запрос о добавлении товара в корзину прилетел на какой-то фронтенд в каком-то дата-центре. Представим, что все наши дата-центры в огне, мы не смогли обработать запрос и вернули ошибку пользователю. Пользователь ушёл, маркетологи прибежали и настучали за недоступность сервиса. 

Да, по CAP-теореме все запросы обработать не можем. Но маркетологи-то злые и не понимают.

Что делать?
\begin{enumerate}
\i Добавил в корзину --- не значит купил. Выдавать ошибку не надо, просто сделать хоть что-то. Запрос сохраняется же, обработаем позже.
\i Корзина неконсистентна. Добавляем товары в корзину на разных вкладках, в разное время, и она может быть в разных состояниях.
\i Корзина всегда доступна на запись. Главное --- записать, синхронизируем потом.
\end{enumerate}
Это похоже на AP-систему.

Что хотим от системы?
\begin{enumerate}
\i Key-Value хранилище (реляционная БД не всегда нужна, для реализации корзины вполне достаточно key-value: пользователь --- ключ).
\i Шардирование (размазывать данные по узлам, не хранить всё в одном месте) \& репликация.
\i Всегда доступны на запись.
\i Версионирование values (чтобы понимать, какой из наборов данных настоящий).
\i Децентрализована.
\i Масштабирование без деградаций.
\end{enumerate}

Это называется Amazon DynamoDB.

\paragraph{Строим БД}

Хэш-таблица или дерево? Хэши, конечно. Как вообще распределять и параллелить деревья?

Пользователь имеет id в диапазоне $[0, R]$. Пусть есть сервера $S_1, \dots, S_N$. Каждому серверу сопоставляем случайное число $T_1, \dots, T_N$.
Данные из диапазона $[T_i, T_{i+1}]$ будем хранить на сервере $S_i$.

Пусть приходит новый сервер $S_{N+1}$ с числом $T_{n+1}$. Вспомним обычную хэш-таблицу. Перехэширование полностью переписывало все данные. Мы не можем себе это позволить. Поэтому смотрим, куда попадает $T_{N+1}$, забираем часть данных от текущей дуги и отправляем на новый.


Итого, переезжает лишь $[\frac{R}{N}]$ данных.
Это называется "консистентный хэш".
Проблемы: 
\i Неравномерность распределения данных
\i Неравномерность использования оборудования

Решение: серверу сопоставляем не ключ, а последовательность ключей $T_{i1}, \dots, T_{1m_i}$. Количество таких кусков $m_i$ зависит от производительности сервера. Этот метод можно рассматривать как "виртуальные сервера". Итак, мы размазали данные более равномерно и учли производительность.

Репликация:
$D$ --- число копий.

В нашей старой системе $R_0 \in T_1$.
Сделаем $R_0 \in T_1, \dots, T_D$.

При добавлении нового сервера нужно поддерживать это. То есть, на новый сервер записать все дубликаты, а с какого-то стереть.

Проблема:
Репликаций может быть меньше. Токены принадлежат виртуальным серверам, а они могут быть на одном физическом.
* Учитываем виртуальность.

К тому же, если у нас есть два дата-центра, хочется хранить реплики в разных дата-центрах, чтобы не было failed domain --- когда отвалился дата-центр, стойка, коммутатор, что-то ещё. 
* Учитываем физические условия.

Для этого надо разметить все наши сервера по этим признакам и выбирать места для дубликатов с учётом этого.

\paragraph{Проблемы и решения}

\begin{table}{|l|l|l|}
Проблема & Техника & Профит \\ \hline
Шардирование & Консистентные хэши & Инкрементальный рост \\
HA (Highly Available) на запись & Векторные часы и reconcilation на чтение & HA и Durability(если прошла запись, то она сохранится, не потеряется)
Обработка отказов & Sloppy Quorums + ... handoffs &
Recovery (узел пропал и вернулся $\Then$ может восстановиться) & anty-entropy и Merkle Trees & Восстановление ...
Membership и Failure detection & gossip negotiation (асинхронно выясняем, кто доступен, а кто нет) & Решили проблему
\end{table}

\begin{enumerate}
\i Формальное описание базы данных (Query language)
key $\to$ value, один запрос --- один ключ
\i ACID 
\begin{enumerate}
\i Atomicity --- атомарность всегда будет, ибо в транзакции только одна операция, и не о чем говорить
\i Consistent --- не будет (если только очень слабая)
\i Isolated --- нет
\i Durability --- eventually (сколько сможем: если ноды упали, но вернулись, сможем получить с них данные)
\end{enumerate}
\i Efficiency --- 99.9\% (перцентиль --- распределение времён ответов, и смотрим на статистику для перцентили 99.9\%)
\i Кастомизация --- дать польщователю настраивать базу данных, tradeoff времени ответа и степени консистентности
\i AAA --- Authorization Authentication ... --- нет. Каждый сервис поднимает свою инсталляцию.
\i Symmetry --- не должно быть выделенных узлов, играющих особую роль, поскольку их отказы будут критичны (т. е. хотим, чтобы узлы имели одинаковую функциональность).
При записи узлы пишут с разной скоростью. Пусть они общаются между собой, делая вместе лучше (Аналог P2P-сетей).
\i Гетерогенность --- учитывать ...
\end{enumerate}
HA на запись
API: put(key, value, ctx)
get(key) -> [(v_1, ctx_1), \dots, (v_n, ctx_n)]

У нас может быть неконсистентность, поэтому иногда БД не сможет различить, что наод вернуть. Поэтому она возвращает несколько вариантов, в надежде на то, что пользовательская программа сможет найти нужное ей значение по контексту ctx.

Можно сделать ctx=версия (версионирование)
Но проблема в линейности этого версионирования.
Пусть прилетели 2 запроса put(k, v_1, x) и (k, v_2, y) на два узла, между которыми не было соединения на момент обработки запроса. Когда связность вернётся, придётся решить, какая из записей настоящая.

Заставим решать проблему БД.
Если выкинуть какой-то ключ (случайно, по записям системных часов), нарушаем Durability. Но есть случаи, когда БД сможет различить, тогда норм.
Если не можем решить на уровне БД, заставляем решать пользовательское приложение.

Векторные часы (воспоминание из курса параллелочек).
\begin{enumerate}
\i int
$p_1$ с часами $c_1$и $p_2$ с часами $c_2$. При получении сообщения с меткой времени $ctx$ процессом его часы $c_i = max(ctx, c_2)+1$. 
Проблема в том, что много информации теряется.

\i vector
Каждый процесс хранит часы каждого из других процессов в виде вектора, и ctx=этот вектор. Происходит событие --- увеличиваем свои часы. При получении сообщения берём максимум из позиций, которые не свои, а свою увеличиваем на 1 (так обеспечиваем транзитивность).

Могут быть проблемы, если вектора становятся длинными (чтоы понять, кто более новый, надо сортировать вектора, это за NlogN). Это решается ...
\i matrix
Тоже есть, вспоминаем сами.
\end{enumerate}

Как пользовательская программа может разрешать неоднозначности?
Если в корзину можно только добавлять, то можно просто объединить списки продуктов и сказать потом базе, что на самом деле всё вместе (cделать put нового значения). 
База сама поймёт по векторным часам, что то значение новое, а предыдущие несколько --- старые.
Это и есть reconcilation --- обновление значений.
При этом при get база может смотреть, у кого старые значения, и говорить им, что надо обновить.

Обработка отказов.
N на самом деле D.
Если $Q>\frac{N}{2}$, пересечение кворумов будет непусто, и потери данных не произойдёт.
Будем собирать отдельные кворумы на чтение и на запись. Если $|Q_R| + |Q_W|>N$, последнюю запись всегда будет видно.

Sloppy Quorums означает, что N не фиксировано.Если сломались все ноды $S_1, \dots, S_D$, пишем на $S_{D+1}$ и на другие, до которых дотянемся. Но при этом если сразу же после такой записи сделаем чтение, записи можем не увидеть: если ноды поднялись, то будем читать с них, а на них нету информации (ещё не синхронизировались). То есть, можем получить старое значение, новое или оба сразу.

Чужие владельцы выставляют на не свои записи флаг foreign и периодически сообщает $T_1, \dots, T_D$, что нужно их забрать. Это и есть Handoff (вспомните помогание в lock-free алгоритмах).

Recovery.
$T_1, T_2$ с общим Range $R_0$. В какой-то момент $T_2$ решает, что надо синхронизироваться. Обмениваться всеми данными по сети долго (может быть, они даже согласованы, тогда ничего делать не надо).
Merkle Trees --- деревья хэшей. 

На лекции $h_i$ обозначалось значение хэш-функции, а не сама функция, сама функция одинакова. Обмениваемся хэшами корня дерева, если совпали, ОК, если нет, обмениваемся хэшами на уровень ниже. Так можно вполне быстро понять, что именно поменялось.
Это допускает асинхронную синхронизацию для ...
Это используется в Bitcoin, Ethereum, magnet в торрентах.

Gossip Protocols
Кольцо с $T_1, T_2, T_3$. Есть клиент $C_1$ и range ключей $R_0$. Тогда всегда $C_0$ обращается к $T_1$, а он потом на $T_2, T_3$ распределяет. Если $T_1$ недоступен, все идут в $T_2$. Это гарантирует нам бо'льшую упорядоченность, и у нас меньше мороки с синхронизацией.
Проблема: может возникнуть bottleneck. Пусть пришла куча клиентов на $T_1$. Он может стать перегружен. 
Поэтому пусть клиент обращается к любому из $T_1, T_2, T_3$. Но клиент не знает о range, он пойдёт к любому узлу. 
? А те уже распределят между собой.

Пусть оператор пришёл и сказал о новой топологии сети. Любому узлу. А узлы между собой должны распределить новую информацию.
...
Пусть пришёл узел. 
Узлы будут асинхронно рассказывать друг другу о новой топологии. В итоге все узнают о новости примерно за логарифм.

Возьмём несколько нод, пометим звёздочками. Будем говорить, что это узлы-информаторы, которые знают самые последние новости, и о новой топологии надо спрашивать сначала их. Потеряли в симметрии, но это внутренний трафик, на него можно забить.
Это называется seed nodes. 

Всё!

Как быстро мы работаем?
Для записи клиент ищет любую ноду, та ищет $|C_W|$ нод и пишет на них.
O(ping+flush*D)

Чтение --- то же самое, только кворум на чтение O(ping).

Кастомизация: меняем значения
$|C_W|$ = 2-3, $|C_W|$ = 2-3, $D$ = 3-4

Консистентное хэширование.
вариант S1 --- см. выше.
Проблемы: неодинаковые куски, неоднородные хэши (получаются "горячие участки").

Вариант S2
Делаем кольцо, разбиваем на range-и $R_1, \dots, R_n$ равномерно. Сервер генерирует себе токены $T_1, \dots, T_{m_i}$. Токен попадает в какой-то range $\Then$ сервер берёт себе этот range. Если какие-то куски остались непокрытыми, надо было сгенерировать больше токенов.

Вариант S3. Узел выбирает себе range-и исходя из загруженности этих range.
\end{document}
