\input{boilerplate.tex}

\begin{document} 

 \section{Распределённые файловые системы для больших данных}
 Для того, чтобы обрабатывать большие данные, их нужно где-то хранить. Вертикальное масштабирование (scale up): покупаем всё бо'льшие диски. Ясно, что это не всегда возможно/целесообразно. Горизонтальное масштабирование (scale out): берём несколько серверов и объединяем в систему. Об этом и пойдёт речь в курсе.
 
 Родоначальник -- GFS (Google File System). Есть статья, где люди из Google объясняют, как они реализовали на имеющемся железе ФС, удовлетворяющую их нуждам для Больших Данных (просто распределённые ФС уже были, а вот для обработки Больших Данных не было).
 
 \subsection{Специфика BigData}
 \begin{itemize}
 \i Большие файлы ($>>1$ GB)
 \i Commodity hardware (обычные сервера, которые производятся и покупаются в больших количествах, и надёжностью особой не отличаются).
 \i "Write once, read many"
  \begin{itemize}
  	\item в основном --- последовательные чтения
  	\i append
    \i нет потребности в random write
  	\i не удалямем данные, а записываем новые (то есть, в серединку файла нельзя записать что-нибудь, придётся создать новый файл и удалить старый).
  \end{itemize}
 \end{itemize}
 Эти требования связаны со спецификой Google: робот скачивает интернет, потом его обрабатывают, в основном -- последовательно, в основном -- чтения.

 \subsection{Архитектура}
\begin{itemize}
\i Master server, хранящий список файлов и их метаданные
\i Сами файлы разбиты на блоки(чанки).
 Блоки хранятся на Chunk servers: обычные сервера (commodity hardware) с обычной local Linux FS. А Master Server собирает это всё в единую ФС, раздаёт chunk-серверам команды, следит за работоспособностью. Запросы приходят на Master-сервер, а данные отдаются напрямую с chunk-серверов (иначе master-сервер будет бутылочным горлышком).
\i Данные и мметаданные хранятся раздельно
\end{itemize}

GFS был в виде статьи и закрытой реализации от Google. Yahoo на тот момент написал HDFS (Hadoop Distributed File System) на Java и выкатил в OpenSource.
Master server == Name node. Chunk server == Data node. chunk == block

TODO: скопипастить со слайда Роли компонент.
\begin{itemize}
 \i Namenode -- NN (master server)
 \begin{itemize}
  \i хранит metadata в памяти (для быстрого доступа)
  \i поддерживает список доступных Datanode
 \end{itemize}
 \i Datanode -- DN (chunk server)
 \begin{itemize} 
  \i хранит блоки (чанки) в локальной FS
  \i оповещает Namenode о состоянии (heartbeat, раз в 3-5 секунд)
  \i оповещает Namenode о списке своих блоков (примерно каждые полчаса)
  \i получает инструкции от Namenode
 \end{itemize}
\end{itemize}

Размер блока
В обычных ФС обычно 4 Кб. 
Зачем вообще разбивать на блоки? Почти любой файл влезет на большой диск. 

Если блок слишком большой, то:
\begin{enumerate}
	\i Чем больше блок -- тем больше фрагментация.
	\i Хуже параллелизм.
\end{enumerate}
Если блок слишком маленький, то: 
\begin{enumerate}
	\i Namenode хочет хранить всё в памяти. Каждый блок. Не влезет, и операции будут не очень быстрыми (если и влезет). 
	\i С каждым блоком хранится чексумма, и она создаёт накладные расходы. 
	\i Чаще всего читаем последовательно. Последовательное чтение быстро. Если блоки слишком маленькие, мы этим преимуществом не воспользуемся. Надо, чтобы время seek'а было мало по сравнению со временем последовательного чтения
	\i Обращение к Datanode тоже занимает время. 
\end{enumerate}

Проблема: очень маленькие файлы (меньше размера блока).
Физического ограничения, что меньше размера блока создавать файлы нельзя, нет. Поэтому мелкий файл будет занимать столько места на диске, сколько он и есть (+метаданные), но будет занимать отдельный блок на Namenode. Если таких мелких файлов очень много:
\begin{enumerate}
	\i трата оперативной памяти Namenode и повышенная нагрузка на него
	\i хуже скорость чтения
\end{enumerate}
Вывод: лучше склеивать мелкие файлы.

Обычно размер блока 64 MB, 128 MB.

\subsection{Отказоустойчивость}
Решение -- репликация.

Считаем, что места у нас много, поэтому можем не применять хитрых алгоритмов, а просто дублировать информацию.

\textbf{Фактор репликации} -- число копий каждого блока. Обычно 3. Копии распределяются по нодам с учётом топологии сети.

 \href{https://youtu.be/-hE8jkyfqMw?t=2m55s}{Видео про дата-центр}
 
Например, плохо дублировать файл на серверах одной и той же стойки, ибо у всей стойки может сломаться маршрутизатор. 
 
\subsection{Чтение из HDFS}
TODO: Вставить картинку
Namenode ещё проверяет права доступа, наличие такого файла, папки, и проч.

\subsection{Запись в HDFS}
Вставить картинку

Клиент передаёт данные на одну Datanode, на которую ему сказал писать Namenode. Потом datanode'ы между собой разбираются, как делать репликацию. После того, как репликация будет произведена, клиенту приходит подтверждение. 

Если процессе записи datanode падает, ack пакет не приходит вовремя, клиентская библиотека сообщает об этом Namenode, Namenode выдаёт новый идентификатор блока (чтобы старый блок, если он частично записался, был помечен как мусор и удалён), и запись начинается снова.
Если, например, данные были записаны на 2 ноды, а третья упала, Namenode даёт приказ продублировать с какой-то из уже записанных нод, а клиенту говорят, что запись завершена. 

Заметим: общение между нодами происходит внутри кластера, где соединение довольно быстрое.

Недостатки:
\begin{enumerate}
	\i нет random reads/writes
	\i не подходит для low-latency приложений (например, key-value хранилища, для систем реального времени, для случаев, когда пользователю требуется быстрый ответ (например, открыть документ))
	\i Проблема мелких файлов (картинки хранить не получится)
	\i Namenode -- единая точка отказа. 
	Поэтому Namenode берётся дорогой и отказоустойчивой, питание, сеть и прочее дублируются, а сама она окружается теплом и заботой. Но лучше резервировать.
\end{enumerate}

\subsection{Secondary Namenode}
TODO: вставить картинку

Сервер, хранящий реплику какого-то состояния Namenode.
Namenode помимо непосредственных изменений памяти пишет лог, и периодически пишет слепки памяти (например, можно выключить её с сохранением состояния и включить потом). Но передавать слепки памяти накладно, поэтому при помощи какой-нибудь системы ротации логи пачками пересылаются на Secondary Namenode, где поддерживается такое же состояние (с точностью до последнего пакета). Secondary Namenode пишет свои слепки памяти периодически, и может их отправлять на Namenode, чтобы следующий старт был с более актуального состояния (и заодно Namenode не надо этим заниматься).

Если Namenode упала надолго, вручную вводится в строй Secondary Namenode как горячая замена (слегка отстающая, впрочем).

Если primary и secondary стоят физически рядом, можно просто подключать диск по NFS, чтобы автоматически все действия primary дублировались на secondary.

Нужно заметить, что эта система не очень приспособлена именно для горячей замены. Для этого есть другие.

Например, федерация Namenode: разбиваем на несколько Namenode, каждая из которых обслуживает свой раздел (напр., /home, /data). Вообще, в данном курсе такое не рассматривается.

\subsection{Клиенты HDFS}
\begin{itemize}
\i Java API
\i CLI
\i C (libhdfs)
\i HTTP (WebHDFS)
\i NFS
\i FUSE (Filesystem in Userspace)
\i Python (pip hdfs использует WebHDFS)
\end{itemize}

\subsection{Command Line Interface}
\begin{itemize}
\i hdfs dfs -<command>
(то же, что hadoop fs -<command>)
\i hdfs dfs –help
\i hdfs dfs -ls <remote>
\i hdfs dfs -put <local> <remote>
\i hdfs dfs -get <remote> <local>
\i hdfs dfs -mv <remote> <remote>
\i hdfs dfs -cp <remote> <remote>*
\i hdfs dfs -cat <remote>; hdfs dfs -text <remote>
\i hadoop distcp hdfs://namenode1/file1 hdfs://namenode1/file2
\end{itemize}

Замечание: -cp работает через клиента, т. е. медленно. hadoop distcp производит копирование внутри кластера (или даже одной ноды). 

Лайфхак: можно так составить конфигурационный файл, чтобы Hadoop считал Ваш компьютер удалённым сервером. Полезно для отладки.

\section{MapReduce}
Были действия,которые совершают над данными. Для каждого конкретного случая писали свой велосипед. Google же заметил сходство и написал универсальную реализацию.

\begin{enumerate}
 \i map
  $f: T \rightarrow [S]=[(k, v)]$.
  Функция применяется поэлементно к входным данным и может выполняться параллельно.
  
  Выходные данные аннотируются ключом.

 \i shuffle \& sort
 
  Данные перераспределяются по редьюсерам так, чтобы значения с одним ключом обязательно попали к одному редьюсеру, и при этом сами ключи были отсортированы.
  
 \i reduce
  $g:k, [v] \rightarrow [(k', v')]$.
\end{enumerate}

Пример:
Лог веб-сервера. Хотим посчитать количество уникальных посетителей, сколько раз каждый пользователь посетил сайт. Различать пользователей можно по uid, который есть в логе. 
\begin{enumerate}
 \i map: выделяем из строки лога uid.
  $f: line->(uid, NULL)$
 \i shuffle\&sort
  Сортируем по uid. 
 \i reduce В сортированном списке количество уникальных элементов считается просто: количество смен ключа при последовательном чтении.
 $g: uid, []_n \rightarrow uid, n$
 $g: uniq -c$ --  утилита unix.
 
\end{enumerate}
\mi{bash}{cat access.log | ./get_uid | sort | uniq.c}

\subsection{Сравнение MPI и MapReduce}
MPI: 
Общее хранилище на SAN(Storage Area Network): данные хранятся отдельно, обрабатываются отдельно, любой узел имеет доступ к данным в равной мере
CPU-intensive (дефицитный ресурс --- процессор).


Минусы MPI.

Плюс: Задачи решаются за прогнозируемое время.


...
Вместо простой разбивки на блоки фиксированного размера вводится разбивка на сплиты: блоки, учитывающие внутреннее строение файла (например, для текстового файла -- по строкам). Поэтому сплит может быть больше по размеру, чем блок.

При отказе оборудования запускаем ту же задачу на другом сервере. Желательно, на том, где данные уже есть. 

\section{14.11.17}
grep на стадии map ищем, всё.

group by
map ничего, sort сортирует по ключу, reduce просто собирает куски с одинаковым ключом в группы.

reduce=sum
Выдаём промежуточно сумму.
reduce=avg
Выдаём сумму и количество. Тогда можно восстановить правильное среднее.
reduce=median
можем сжать повторяющиеся значения в пары (количество, ключ)
Но комбайн плохо всё равно работает.


Обратный индекс
В мапе к слову приписываем количество его встреч, свели задачу к предыдущей.

В целом кажется, что в \wikilangref{en}{Википедии}{MapReduce} довольно годно написано.

\section{Леонов, Отказоустойчивые распределённые системы}
wait-free
Независимо от того, какие сообщения приходят тебе и доходят ли твои сообщения до других, ты за константное время совершаешь прогресс.

Асинхронная SM. Барьер - примитив синхронизации.

1. Пусть два синхронизатора с разницей 2. Тогда 

2. Пусть получили сообщение от процессора в такте x-1. Но мы могли перейти в такт x только получив все сообщения от провессоров в такте x-1.

Разрешаем f отказов. Потоки отправляют свои значения. Когда получит n-f сообщений, выбирает f из n-f чисел детерминированной функцией, например, минимальных. И среди них хотя бы одно будет общим.

Семинар
Hive. Люди из Facebook заметили, что типичные задачи работы с хранилищем удобно представлять в чём-то, похожем на реляционную алгебру.

Над кластером строится Hive metastore, в котором хранится информация о таблицах: из каких файлов она состоит, а где они хранятся знает HDFS.
Metastore: table $\rightarrow$ location (папка HDFS) + информация о таблице (колонки, их типы, формат хранения).
То есть, это аналог Namenode. Само оно устроено в виде обычной БД (SQLite, \dots).

Driver Hive: HiveQL $\rightarrow$ MapReduce Jobs.
Преобразуем команду, похожую на обычный SQL-запрос, в каскад MapReduce'ов.


\section{Отказоустойчивые распределённые системы}
Руководство компании принимает спонтанные решения.
Алексей один, ему некому вредить. Поэтому и не смешно.
SSM -- та модель, которую используют при написании многопоточных программ. Точки синхронизации---барьеры--- создают общие для всех потоков часы.
SMP --- распределённая база данных с синхронизатором. Синхронизатор берёт всё асинхронное на себя, а остаётся синхронная часть.

Консенсус (задача о византийских генералах). Есть процессы и числа. Из всех предложенных чисел нужно выбрать одно. В ситуации с отказом число должно быть предложено не отказавшим процессом. 

GS == Global State

Сортировать надо не лексикографически(контрпример 01 10). Говорят, что достаточно выписать n состояний $0\dots0, 10\dots0, 110\dots0, 1\dots1$, которые из $I_1$  приводят к $I_2$. 


Terminated  .... Есть распределённая БД, надо, чтобы все узлы, реализующие её, получили значение, которое нужно записать

Есть цепочка банков, переводим с одного счёта на другой через всю цепочку.

CP -- не реагируем на записи, всегда возвращаем начальное состояние. 
AP -- Будем держать локальную базу данных и не общаться. Каждый поток возвращает своё.
CA -- Выделяем сервер, и обращаемся к нему как клиенты.

проксировать == переслать


\section{Сем}
ZooKeeper
\begin{enumerate}
	\i PDF
	\i Официальная документация
\end{enumerate}

Основная задача --- средство синхронизации (хотя можно хранить в нём что-то как в ДБ).

Chubby -- система Google для синхронизации хостов. 

Модель данных: древовидная.
Узел может делать \mi{text}{get, get_data, get_children, set_data, create, remove, sync}.

setData(str, version) -- аналогично CAS.

\section{Прикладная задача}
Есть пользователь, который хочет купить что-то в интернет-магазине. Его запрос о добавлении товара в корзину прилетел на какой-то фронтенд в каком-то дата-центре. Представим, что все наши дата-центры в огне, мы не смогли обработать запрос и вернули ошибку пользователю. Пользователь ушёл, маркетологи прибежали и настучали за недоступность сервиса. 

Да, по CAP-теореме все запросы обработать не можем. Но маркетологи-то злые и не понимают.

Что делать?
\begin{enumerate}
\i Добавил в корзину --- не значит купил. Выдавать ошибку не надо, просто сделать хоть что-то. Запрос сохраняется же, обработаем позже.
\i Корзина неконсистентна. Добавляем товары в корзину на разных вкладках, в разное время, и она может быть в разных состояниях.
\i Корзина всегда доступна на запись. Главное --- записать, синхронизируем потом.
\end{enumerate}
Это похоже на AP-систему.

Что хотим от системы?
\begin{enumerate}
\i Key-Value хранилище (реляционная БД не всегда нужна, для реализации корзины вполне достаточно key-value: пользователь --- ключ).
\i Шардирование (размазывать данные по узлам, не хранить всё в одном месте) \& репликация.
\i Всегда доступны на запись.
\i Версионирование values (чтобы понимать, какой из наборов данных настоящий).
\i Децентрализована.
\i Масштабирование без деградаций.
\end{enumerate}

Это называется Amazon DynamoDB.

\paragraph{Строим БД}

Хэш-таблица или дерево? Хэши, конечно. Как вообще распределять и параллелить деревья?

Пользователь имеет id в диапазоне $[0, R]$. Пусть есть сервера $S_1, \dots, S_N$. Каждому серверу сопоставляем случайное число $T_1, \dots, T_N$.
Данные из диапазона $[T_i, T_{i+1}]$ будем хранить на сервере $S_i$.

Пусть приходит новый сервер $S_{N+1}$ с числом $T_{n+1}$. Вспомним обычную хэш-таблицу. Перехэширование полностью переписывало все данные. Мы не можем себе это позволить. Поэтому смотрим, куда попадает $T_{N+1}$, забираем часть данных от текущей дуги и отправляем на новый.


Итого, переезжает лишь $[\frac{R}{N}]$ данных.
Это называется "консистентный хэш".
Проблемы: 
\begin{enumerate}
\i Неравномерность распределения данных
\i Неравномерность использования оборудования
\end{enumerate}

Решение: серверу сопоставляем не ключ, а последовательность ключей $T_{i1}, \dots, T_{1m_i}$. Количество таких кусков $m_i$ зависит от производительности сервера. Этот метод можно рассматривать как "виртуальные сервера". Итак, мы размазали данные более равномерно и учли производительность.

Репликация:
$D$ --- число копий.

В нашей старой системе $R_0 \in T_1$.
Сделаем $R_0 \in T_1, \dots, T_D$.

При добавлении нового сервера нужно поддерживать это. То есть, на новый сервер записать все дубликаты, а с какого-то стереть.

Проблема:
Репликаций может быть меньше. Токены принадлежат виртуальным серверам, а они могут быть на одном физическом.
* Учитываем виртуальность.

К тому же, если у нас есть два дата-центра, хочется хранить реплики в разных дата-центрах, чтобы не было failed domain --- когда отвалился дата-центр, стойка, коммутатор, что-то ещё. 
* Учитываем физические условия.

Для этого надо разметить все наши сервера по этим признакам и выбирать места для дубликатов с учётом этого.

\paragraph{Проблемы и решения}

\begin{tabular}{|l|l|l|}
Проблема & Техника & Профит \\ \hline
Шардирование & Консистентные хэши & Инкрементальный рост \\ \hline
HA (Highly Available) на запись & Векторные часы и reconcilation на чтение & HA и Durability(если прошла запись, то она сохранится, не потеряется)\\ \hline
Обработка отказов & Sloppy Quorums + ... handoffs & Решили проблему \\ \hline
Recovery (узел пропал и вернулся $\Then$ может восстановиться) & anty-entropy и Merkle Trees & Восстановление ... \\ \hline
Membership и Failure detection & gossip negotiation (асинхронно выясняем, кто доступен, а кто нет) & Решили проблему \\ \hline
\end{tabular}

\begin{enumerate}
\i Формальное описание базы данных (Query language)
key $\to$ value, один запрос --- один ключ
\i ACID 
\begin{enumerate}
\i Atomicity --- атомарность всегда будет, ибо в транзакции только одна операция, и не о чем говорить
\i Consistent --- не будет (если только очень слабая)
\i Isolated --- нет
\i Durability --- eventually (сколько сможем: если ноды упали, но вернулись, сможем получить с них данные)
\end{enumerate}
\i Efficiency --- 99.9\% (перцентиль --- распределение времён ответов, и смотрим на статистику для перцентили 99.9\%)
\i Кастомизация --- дать польщователю настраивать базу данных, tradeoff времени ответа и степени консистентности
\i AAA --- Authorization Authentication ... --- нет. Каждый сервис поднимает свою инсталляцию.
\i Symmetry --- не должно быть выделенных узлов, играющих особую роль, поскольку их отказы будут критичны (т. е. хотим, чтобы узлы имели одинаковую функциональность).
При записи узлы пишут с разной скоростью. Пусть они общаются между собой, делая вместе лучше (Аналог P2P-сетей).
\i Гетерогенность --- учитывать ...
\end{enumerate}
HA на запись
API: $put(key, value, ctx)$
$get(key) -> [(v_1, ctx_1), \dots, (v_n, ctx_n)]$

У нас может быть неконсистентность, поэтому иногда БД не сможет различить, что наод вернуть. Поэтому она возвращает несколько вариантов, в надежде на то, что пользовательская программа сможет найти нужное ей значение по контексту ctx.

Можно сделать ctx=версия (версионирование)
Но проблема в линейности этого версионирования.
Пусть прилетели 2 запроса $put(k, v_1, x)$ и $put(k, v_2, y)$ на два узла, между которыми не было соединения на момент обработки запроса. Когда связность вернётся, придётся решить, какая из записей настоящая.

Заставим решать проблему БД.
Если выкинуть какой-то ключ (случайно, по записям системных часов), нарушаем Durability. Но есть случаи, когда БД сможет различить, тогда норм.
Если не можем решить на уровне БД, заставляем решать пользовательское приложение.

Векторные часы (воспоминание из курса параллелочек).
\begin{enumerate}
\i int
$p_1$ с часами $c_1$и $p_2$ с часами $c_2$. При получении сообщения с меткой времени $ctx$ процессом его часы $c_i = max(ctx, c_2)+1$. 
Проблема в том, что много информации теряется.

\i vector
Каждый процесс хранит часы каждого из других процессов в виде вектора, и ctx=этот вектор. Происходит событие --- увеличиваем свои часы. При получении сообщения берём максимум из позиций, которые не свои, а свою увеличиваем на 1 (так обеспечиваем транзитивность).

Могут быть проблемы, если вектора становятся длинными (чтоы понять, кто более новый, надо сортировать вектора, это за NlogN). Это решается ...
\i matrix
Тоже есть, вспоминаем сами.
\end{enumerate}

Как пользовательская программа может разрешать неоднозначности?
Если в корзину можно только добавлять, то можно просто объединить списки продуктов и сказать потом базе, что на самом деле всё вместе (cделать put нового значения). 
База сама поймёт по векторным часам, что то значение новое, а предыдущие несколько --- старые.
Это и есть reconcilation --- обновление значений.
При этом при get база может смотреть, у кого старые значения, и говорить им, что надо обновить.

Обработка отказов.
N на самом деле D.
Если $Q>\frac{N}{2}$, пересечение кворумов будет непусто, и потери данных не произойдёт.
Будем собирать отдельные кворумы на чтение и на запись. Если $|Q_R| + |Q_W|>N$, последнюю запись всегда будет видно.

Sloppy Quorums означает, что N не фиксировано.Если сломались все ноды $S_1, \dots, S_D$, пишем на $S_{D+1}$ и на другие, до которых дотянемся. Но при этом если сразу же после такой записи сделаем чтение, записи можем не увидеть: если ноды поднялись, то будем читать с них, а на них нету информации (ещё не синхронизировались). То есть, можем получить старое значение, новое или оба сразу.

Чужие владельцы выставляют на не свои записи флаг foreign и периодически сообщает $T_1, \dots, T_D$, что нужно их забрать. Это и есть Handoff (вспомните помогание в lock-free алгоритмах).

Recovery.
$T_1, T_2$ с общим Range $R_0$. В какой-то момент $T_2$ решает, что надо синхронизироваться. Обмениваться всеми данными по сети долго (может быть, они даже согласованы, тогда ничего делать не надо).
Merkle Trees --- деревья хэшей. 

На лекции $h_i$ обозначалось значение хэш-функции, а не сама функция, сама функция одинакова. Обмениваемся хэшами корня дерева, если совпали, ОК, если нет, обмениваемся хэшами на уровень ниже. Так можно вполне быстро понять, что именно поменялось.
Это допускает асинхронную синхронизацию для ...
Это используется в Bitcoin, Ethereum, magnet в торрентах.

Gossip Protocols
Кольцо с $T_1, T_2, T_3$. Есть клиент $C_1$ и range ключей $R_0$. Тогда всегда $C_0$ обращается к $T_1$, а он потом на $T_2, T_3$ распределяет. Если $T_1$ недоступен, все идут в $T_2$. Это гарантирует нам бо'льшую упорядоченность, и у нас меньше мороки с синхронизацией.
Проблема: может возникнуть bottleneck. Пусть пришла куча клиентов на $T_1$. Он может стать перегружен. 
Поэтому пусть клиент обращается к любому из $T_1, T_2, T_3$. Но клиент не знает о range, он пойдёт к любому узлу. 
? А те уже распределят между собой.

Пусть оператор пришёл и сказал о новой топологии сети. Любому узлу. А узлы между собой должны распределить новую информацию.
...
Пусть пришёл узел. 
Узлы будут асинхронно рассказывать друг другу о новой топологии. В итоге все узнают о новости примерно за логарифм.

Возьмём несколько нод, пометим звёздочками. Будем говорить, что это узлы-информаторы, которые знают самые последние новости, и о новой топологии надо спрашивать сначала их. Потеряли в симметрии, но это внутренний трафик, на него можно забить.
Это называется seed nodes. 

Всё!

Как быстро мы работаем?
Для записи клиент ищет любую ноду, та ищет $|C_W|$ нод и пишет на них.
O(ping+flush*D)

Чтение --- то же самое, только кворум на чтение O(ping).

Кастомизация: меняем значения
$|C_W|$ = 2-3, $|C_W|$ = 2-3, $D$ = 3-4

Консистентное хэширование.
вариант S1 --- см. выше.
Проблемы: неодинаковые куски, неоднородные хэши (получаются "горячие участки").

Вариант S2
Делаем кольцо, разбиваем на range-и $R_1, \dots, R_n$ равномерно. Сервер генерирует себе токены $T_1, \dots, T_{m_i}$. Токен попадает в какой-то range $\Then$ сервер берёт себе этот range. Если какие-то куски остались непокрытыми, надо было сгенерировать больше токенов.

Вариант S3. Узел выбирает себе range-и исходя из загруженности этих range.
\end{document}
