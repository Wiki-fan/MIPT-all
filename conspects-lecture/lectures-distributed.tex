\input{boilerplate.tex}

\newcommand{\msg}{\texttt}

\begin{document} 

 \section{Распределённые файловые системы для больших данных}
 Для того, чтобы обрабатывать большие данные, их нужно где-то хранить. Вертикальное масштабирование (scale up): покупаем всё бо'льшие диски. Ясно, что это не всегда возможно/целесообразно. Горизонтальное масштабирование (scale out): берём несколько серверов и объединяем в систему. Об этом и пойдёт речь в курсе.
 
 Родоначальник -- GFS (Google File System). Есть статья, где люди из Google объясняют, как они реализовали на имеющемся железе ФС, удовлетворяющую их нуждам для Больших Данных (просто распределённые ФС уже были, а вот для обработки Больших Данных не было).
 
 \subsection{Специфика BigData}
 \begin{itemize}
 \i Большие файлы ($>>1$ GB)
 \i Commodity hardware (обычные сервера, которые производятся и покупаются в больших количествах, и надёжностью особой не отличаются).
 \i "Write once, read many"
  \begin{itemize}
  	\item в основном --- последовательные чтения
  	\i append
    \i нет потребности в random write
  	\i не удалямем данные, а записываем новые (то есть, в серединку файла нельзя записать что-нибудь, придётся создать новый файл и удалить старый).
  \end{itemize}
 \end{itemize}
 Эти требования связаны со спецификой Google: робот скачивает интернет, потом его обрабатывают, в основном -- последовательно, в основном -- чтения.

 \subsection{Архитектура}
\begin{itemize}
\i Master server, хранящий список файлов и их метаданные
\i Сами файлы разбиты на блоки(чанки).
 Блоки хранятся на Chunk servers: обычные сервера (commodity hardware) с обычной local Linux FS. А Master Server собирает это всё в единую ФС, раздаёт chunk-серверам команды, следит за работоспособностью. Запросы приходят на Master-сервер, а данные отдаются напрямую с chunk-серверов (иначе master-сервер будет бутылочным горлышком).
\i Данные и мметаданные хранятся раздельно
\end{itemize}

GFS был в виде статьи и закрытой реализации от Google. Yahoo на тот момент написал HDFS (Hadoop Distributed File System) на Java и выкатил в OpenSource.
Master server == Name node. Chunk server == Data node. chunk == block

TODO: скопипастить со слайда Роли компонент.
\begin{itemize}
 \i Namenode -- NN (master server)
 \begin{itemize}
  \i хранит metadata в памяти (для быстрого доступа)
  \i поддерживает список доступных Datanode
 \end{itemize}
 \i Datanode -- DN (chunk server)
 \begin{itemize} 
  \i хранит блоки (чанки) в локальной FS
  \i оповещает Namenode о состоянии (heartbeat, раз в 3-5 секунд)
  \i оповещает Namenode о списке своих блоков (примерно каждые полчаса)
  \i получает инструкции от Namenode
 \end{itemize}
\end{itemize}

Размер блока
В обычных ФС обычно 4 Кб. 
Зачем вообще разбивать на блоки? Почти любой файл влезет на большой диск. 

Если блок слишком большой, то:
\begin{enumerate}
	\i Чем больше блок -- тем больше фрагментация.
	\i Хуже параллелизм.
\end{enumerate}
Если блок слишком маленький, то: 
\begin{enumerate}
	\i Namenode хочет хранить всё в памяти. Каждый блок. Не влезет, и операции будут не очень быстрыми (если и влезет). 
	\i С каждым блоком хранится чексумма, и она создаёт накладные расходы. 
	\i Чаще всего читаем последовательно. Последовательное чтение быстро. Если блоки слишком маленькие, мы этим преимуществом не воспользуемся. Надо, чтобы время seek'а было мало по сравнению со временем последовательного чтения
	\i Обращение к Datanode тоже занимает время. 
\end{enumerate}

Проблема: очень маленькие файлы (меньше размера блока).
Физического ограничения, что меньше размера блока создавать файлы нельзя, нет. Поэтому мелкий файл будет занимать столько места на диске, сколько он и есть (+метаданные), но будет занимать отдельный блок на Namenode. Если таких мелких файлов очень много:
\begin{enumerate}
	\i трата оперативной памяти Namenode и повышенная нагрузка на него
	\i хуже скорость чтения
\end{enumerate}
Вывод: лучше склеивать мелкие файлы.

Обычно размер блока 64 MB, 128 MB.

\subsection{Отказоустойчивость}
Решение -- репликация.

Считаем, что места у нас много, поэтому можем не применять хитрых алгоритмов, а просто дублировать информацию.

\textbf{Фактор репликации} -- число копий каждого блока. Обычно 3. Копии распределяются по нодам с учётом топологии сети.

 \href{https://youtu.be/-hE8jkyfqMw?t=2m55s}{Видео про дата-центр}
 
Например, плохо дублировать файл на серверах одной и той же стойки, ибо у всей стойки может сломаться маршрутизатор. 
 
\subsection{Чтение из HDFS}
TODO: Вставить картинку
Namenode ещё проверяет права доступа, наличие такого файла, папки, и проч.

\subsection{Запись в HDFS}
Вставить картинку

Клиент передаёт данные на одну Datanode, на которую ему сказал писать Namenode. Потом datanode'ы между собой разбираются, как делать репликацию. После того, как репликация будет произведена, клиенту приходит подтверждение. 

Если процессе записи datanode падает, ack пакет не приходит вовремя, клиентская библиотека сообщает об этом Namenode, Namenode выдаёт новый идентификатор блока (чтобы старый блок, если он частично записался, был помечен как мусор и удалён), и запись начинается снова.
Если, например, данные были записаны на 2 ноды, а третья упала, Namenode даёт приказ продублировать с какой-то из уже записанных нод, а клиенту говорят, что запись завершена. 

Заметим: общение между нодами происходит внутри кластера, где соединение довольно быстрое.

Недостатки:
\begin{enumerate}
	\i нет random reads/writes
	\i не подходит для low-latency приложений (например, key-value хранилища, для систем реального времени, для случаев, когда пользователю требуется быстрый ответ (например, открыть документ))
	\i Проблема мелких файлов (картинки хранить не получится)
	\i Namenode -- единая точка отказа. 
	Поэтому Namenode берётся дорогой и отказоустойчивой, питание, сеть и прочее дублируются, а сама она окружается теплом и заботой. Но лучше резервировать.
\end{enumerate}

\subsection{Secondary Namenode}
TODO: вставить картинку

Сервер, хранящий реплику какого-то состояния Namenode.
Namenode помимо непосредственных изменений памяти пишет лог, и периодически пишет слепки памяти (например, можно выключить её с сохранением состояния и включить потом). Но передавать слепки памяти накладно, поэтому при помощи какой-нибудь системы ротации логи пачками пересылаются на Secondary Namenode, где поддерживается такое же состояние (с точностью до последнего пакета). Secondary Namenode пишет свои слепки памяти периодически, и может их отправлять на Namenode, чтобы следующий старт был с более актуального состояния (и заодно Namenode не надо этим заниматься).

Если Namenode упала надолго, вручную вводится в строй Secondary Namenode как горячая замена (слегка отстающая, впрочем).

Если primary и secondary стоят физически рядом, можно просто подключать диск по NFS, чтобы автоматически все действия primary дублировались на secondary.

Нужно заметить, что эта система не очень приспособлена именно для горячей замены. Для этого есть другие.

Например, федерация Namenode: разбиваем на несколько Namenode, каждая из которых обслуживает свой раздел (напр., /home, /data). Вообще, в данном курсе такое не рассматривается.

\subsection{Клиенты HDFS}
\begin{itemize}
\i Java API
\i CLI
\i C (libhdfs)
\i HTTP (WebHDFS)
\i NFS
\i FUSE (Filesystem in Userspace)
\i Python (pip hdfs использует WebHDFS)
\end{itemize}

\subsection{Command Line Interface}
\begin{itemize}
\i hdfs dfs -<command>
(то же, что hadoop fs -<command>)
\i hdfs dfs –help
\i hdfs dfs -ls <remote>
\i hdfs dfs -put <local> <remote>
\i hdfs dfs -get <remote> <local>
\i hdfs dfs -mv <remote> <remote>
\i hdfs dfs -cp <remote> <remote>*
\i hdfs dfs -cat <remote>; hdfs dfs -text <remote>
\i hadoop distcp hdfs://namenode1/file1 hdfs://namenode1/file2
\end{itemize}

Замечание: -cp работает через клиента, т. е. медленно. hadoop distcp производит копирование внутри кластера (или даже одной ноды). 

Лайфхак: можно так составить конфигурационный файл, чтобы Hadoop считал Ваш компьютер удалённым сервером. Полезно для отладки.

\section{MapReduce}
Были действия,которые совершают над данными. Для каждого конкретного случая писали свой велосипед. Google же заметил сходство и написал универсальную реализацию.

\begin{enumerate}
 \i map
  $f: T \rightarrow [S]=[(k, v)]$.
  Функция применяется поэлементно к входным данным и может выполняться параллельно.
  
  Выходные данные аннотируются ключом.

 \i shuffle \& sort
 
  Данные перераспределяются по редьюсерам так, чтобы значения с одним ключом обязательно попали к одному редьюсеру, и при этом сами ключи были отсортированы.
  
 \i reduce
  $g:k, [v] \rightarrow [(k', v')]$.
\end{enumerate}

Пример:
Лог веб-сервера. Хотим посчитать количество уникальных посетителей, сколько раз каждый пользователь посетил сайт. Различать пользователей можно по uid, который есть в логе. 
\begin{enumerate}
 \i map: выделяем из строки лога uid.
  $f: line->(uid, NULL)$
 \i shuffle\&sort
  Сортируем по uid. 
 \i reduce В сортированном списке количество уникальных элементов считается просто: количество смен ключа при последовательном чтении.
 $g: uid, []_n \rightarrow uid, n$
 $g: uniq -c$ --  утилита unix.
 
\end{enumerate}
\mi{bash}{cat access.log | ./get_uid | sort | uniq.c}

\subsection{Сравнение MPI и MapReduce}
MPI: 
Общее хранилище на SAN(Storage Area Network): данные хранятся отдельно, обрабатываются отдельно, любой узел имеет доступ к данным в равной мере
CPU-intensive (дефицитный ресурс --- процессор).


Минусы MPI.

Плюс: Задачи решаются за прогнозируемое время.


...
Вместо простой разбивки на блоки фиксированного размера вводится разбивка на сплиты: блоки, учитывающие внутреннее строение файла (например, для текстового файла -- по строкам). Поэтому сплит может быть больше по размеру, чем блок.

При отказе оборудования запускаем ту же задачу на другом сервере. Желательно, на том, где данные уже есть. 

\section{14.11.17}
grep на стадии map ищем, всё.

group by
map ничего, sort сортирует по ключу, reduce просто собирает куски с одинаковым ключом в группы.

reduce=sum
Выдаём промежуточно сумму.
reduce=avg
Выдаём сумму и количество. Тогда можно восстановить правильное среднее.
reduce=median
можем сжать повторяющиеся значения в пары (количество, ключ)
Но комбайн плохо всё равно работает.


Обратный индекс
В мапе к слову приписываем количество его встреч, свели задачу к предыдущей.

В целом кажется, что в \wikilangref{en}{Википедии}{MapReduce} довольно годно написано.

\section{Леонов, Отказоустойчивые распределённые системы}

Надо смириться с тем, что отказы в распределённых системах будут происходить, и планировать их.

\subsection{Модель распределённой системы}

A distributed system is one in which the failure of a computer you didn’t even know existed can render your own computer unusable.

--- Leslie Lamport

Для дальнейшей работы нам нужна модель, на базе которой мы будем работать. Интуиция подсказывает, что в системе должны быть как-то представлены работающие процессоры и средство коммуникации. Осталось определиться со свойствами.

Cуществует две степени свободы:
\begin{el}[ul]
@ коммуникация: Shared memory vs Message Passing
@@ Shared Memory
@@@ Система состоит из вычислительных узлов, объединённых в сеть
@@@ Вычислительные узлы не имеют общей памяти и взаимодействуют только через передачу сообщений
@@ Message Passing
@@@ Процессоры взаимодействуют через доступ к общей памяти
@ выполнение: Synchronous vs Asynchronous
@@ Synchronous
@@@ Вычисления происходят тактами
@@@ Узлы имеют локальные монотонные часы, часы несогласованы, но есть ограничение на отклонение часов друг от друга
@@@ (message passing) Задержки передачи в сети конечные и укладываются в 1 логический такт
@@@ (shared memory) Все обращения одним процессором к общей памяти укладываются в 1 логический такт
@@ Asynchronous
@@@ Узлы имеют локальные монотонные часы, часы несогласованы, отклонение между часами неограничено
@@@ (message passing) Задержки передачи в сети конечные, но неограниченные
@@@ (shared memory) Задержки доступа к общей памяти непредсказуемы, но конечны
\end{el}

wait-free
Независимо от того, какие сообщения приходят тебе и доходят ли твои сообщения до других, ты за константное время совершаешь прогресс.

\begin{stmt}
MP $\Iff$ SM
\end{stmt}
\begin{proof}
\begin{itemize}
\i MP $\Then$ SM
Будем использовать адресное пространство вычислительных
узлов как общее адресное пространство.
Операции store() и load() будем эмулировать через отправку
сообщения соответствующему узлу
\i SM $\Then$ MP
Разобъём общее адресное пространство на N непересекающихся
множеств. В множестве i зарезервируем N буферов $B_{ij}$ и на базе буферов реализуем SPSC-очереди. Отправку сообщения процессом $i$ процессу $j$ будем эмулировать через запись в очередь $B_{ij}$. Получение сообщения --- через чтение из очереди $B_{ij}$.
\end{itemize}
\end{proof}

\begin{stmt}
A $\Iff$ S
\end{stmt}
\begin{proof}
\begin{itemize}
\i S $\Then$ A
Синхронная модель – особый случай асихронной. Т. о., эмуляция не требуется.
\i A $\Then$ S
Для MP систем нам потребуется специальный примитив Synchronizer, который позволит эмулировать тактовость синхронной системы.
Для SM систем мы просто можем использовать барьеры.
\end{itemize}
\end{proof}

\subsection{Формальное определение: AMP}

Система состоит из n процессов $p_i$, расположённых на n
процессорах. $C_{ij}$ --- канал связи между процессами и $m_{ij}$ --- сообщение от $p_i$ для $p_j$.
Вычисления асинхронные, т.е. процесс в случайный момент времени выполняет внутренний шаг алгоритма или производит отправку/получение сообщения. Глобальных часов нет, отправка и получение сообщения не блокируют процесс.

Выполнение процесса $p_i$ состоит из набора упорядоченных событий $e_i^x$.
События бывают 3 типов: send(m), receive(m) или внутреннее событие.

Заданы три отношения порядка на множестве событий:
Линейный порядок на событиях процесса $p_i$:
$H_i = (\bigcup\limits_x e_i^x \rightarrow i)$.
отношение порядка $\xrightarrow[]{msg}$ между отправкой и получением сообщения:
send(m) $\xrightarrow[]{msg}$ receive(m)
Их замыканием будет отношение $H = (\bigcup e, \to)$
$e_i^x \to e_j^y \Iff 
\begin{sqcases}
e_i^x \xrightarrow[]{i} e_j^y \\
e_i^x \xrightarrow[]{msg} e_j^y \\
\exists e_k^z: e_i^x e_i^x \to e_k^z \land e_k^z \to e_j^y
\end{sqcases}$

\subsection{Синхронизатор}
Синхронизатор используется для эмуляции синхронной системы поверх асинхронной. При помощи синхронизатора будем эмулировать методы send(m), receive(m), а также генерацию событий начала нового такта.

Потребуем, чтобы в рамках одного эмулируемого такта процесс $p_i$
отправлял процессу $p_j$ всегда ровно одно сообщение:
\begin{ol}
\item Если эмулируемый процесс $p_i$ не отправлят $p_j$ ни одного
сообщения, то будем генерировать пустое сообщение.
\item Если эмулируемый алгоритм $p_i$ отправляет несколько сообщений
$p_j$, то отправим одно сообщение --- комбинацию исходных.
\end{ol}

Номер такта:
\begin{el}[ul]
@ Синхронизатор начинает в состоянии такт 0.
@ Когда синхронизатор получает сообщения от всех соседей, он
переходит в такт x + 1
\end{el}
\begin{stmt}
Простой синхронизатор корректен
\end{stmt}
\begin{proof}
\begin{el}[ul]
@ Разница в тактах между любыми двумя синхронизаторами не
больше 1. 

 Пусть два синхронизатора с разницей 2. Тогда 
@ Синхронизатор в такте x может получать сообщения от соседей
находящихся только в тактах x или x + 1

 Пусть получили сообщение от процессора в такте $x-1$. Но мы могли перейти в такт $x$ только получив все сообщения от процессоров в такте $x-1$. Противоречие.\qedhere
\end{el}
\end{proof}

\begin{note}
Всё описанное верно только для систем без отказов. Дальше мы увидим, что реализация синхронзатора при наличии отказов может быть невозможна. Т.о., синхронные системы имеют б\'{о}льшие возможности, чем асинхронные.
\end{note}

\subsection{Модель отказов}
Отказывать могут процессы или сеть. Мы ограничимся рассмотрением отказов процессов, значительную часть отказов сети можно свести к отказм процессов.

Будем называть систему f-failure tolerant, если при отказе до f процессов система продолжает удовлетворять заявленным свойствам.

\begin{el}[ul]
@ Доброкачественные (benign) отказы
@@ Fail-stop: После наступления события $e_i^x$ процесс $p_i$ перестаёт выполнять какие-бы то ни было действия. Дополнительно, остальные процессы могут узнать об этом событии при помощи дополнительной абстракции.
@@ Crash: После события $e_i^x$ процесс $p_i$ перестаёт выполнять любые действия. Механизма извещения других процессов не предусмотрено.
@@ Omission: Работающий процесс можеть терять полученные сообщения до их обработки или забывать отправлять некоторые сообщения. Различают особые случаи: send omission и receive omission.
@ Злокачественные (malign) отказы
@@ Византийский (Byzantine) отказ: процессы с византийским отказом могут демонстрировать произвольное поведение. Чаще всего под этим понимается поведение злоумышленника, получившего полный контроль над одним из процессов и пытающегося навредить системе.
\end{el}

\begin{stmt}
В приведённой классификации одни отказы являются частным случаем других:
Fail-stop $\subset$ Crash $\subset$ (Send omission | Receive omission) $\subset$ General omission $\subset$ Byzantine
\end{stmt}
\begin{proof}
\begin{el}[ul]
@ Fail-stop отказы являются частным случаем Crash отказов, т.к. подразумевают наличие абстракции нотифкации об отказе
@ Crash является частным случаем General omission отказов, т.к. может быть описан потерей всех сообщений
@ Потеря сообщений входит в понятие <<произвольное поведение>>, поэтому Omission отказы являются частным случаем византийских.
\end{el}
\end{proof}

\subsection{Консенсус}
Постановка задачи:

\begin{el}[ul]
@ Процессы
@@ Отказы 
 В системе из n процессов разрешим отказывать не более чем f процессам
@@ Тип отказов Будем разрешать только один тип отказа (например, crash)
@@ Процесс может упасть в середине логического шага и (например) отправить часть сообщений из планируемых
@@ В асинхронной системе невозможно понять, сломался ли получатель сообщения, или оно ещё не доставлено
@ Сеть
@@ Сеть надёжна, т.е. не теряет сообщения
@@ Между любой парой процессов есть связанность
@ Консенсус
@@ Значением консенсуса будем считать boolean или любой другой тип
\end{el}

Система состоит из N процессов, и не более f процессов могут испытать отказ класса $\psi$. Каждый процесс получает на вход значение, все исправные процессы должны выбрать одно число из предложенных.

Формально:
\begin{el}[ul]
@ \bf{Agreement} Все исправные процессы выбирают одно и то же число
@ \bf{Validity} Процессы выбирают число из предложенных
@ \bf{Termination} Каждый процесс рано или поздно примет решение
\end{el}

В случае отсутствия отказов задача решается тривиально:
\begin{el}[ul]
@ Процесс рассылает всем своё значение
@ Когда процесс знает значения всех остальных процессов, он принимает решение
@ Решение приниматеся при помощи заранее известной функции, например min(values) или majority(values)
\end{el}

Разрешаем f отказов. Потоки отправляют свои значения. Когда получит n-f сообщений, выбирает f из n-f чисел детерминированной функцией, например, минимальных. И среди них хотя бы одно будет общим.


\section{Отказоустойчивые распределённые системы}

\subsection{Объяснения шуток}
\begin{el}[ul]
@ Что смешного в фразе "Руководство компании™ демонстрирует византийскую форму отказа"? \\
Руководство компании принимает спонтанные решения.
@ Почему не смешна фраза "Алексей демонстрирует признаки византийского отказа"? \\
Алексей один, ему некому вредить.
@ Синхронные вычисления требуют глобальных часов для
управления старта тактами?
@ Приведите пример SSM (Synchronous shared memory) системы\\
SSM --- та модель, которую используют при написании многопоточных программ. Точки синхронизации --- барьеры --- создают общие для всех потоков часы.
@ Приведите пример SMP (Synchronous message passing) системы
Распределённая база данных с синхронизатором. Синхронизатор берёт всё асинхронное на себя, а остаётся синхронная часть.
\end{el}

\subsection{Теорема Фишера, Линч и Патерсона}
\begin{thm}[FLP impossibility]
В асинхронной системе, построенной на передаче сообщений (AMP), не существует алгоритма консенсуса, допускающего Crash хотя бы одного процесса.
\end{thm}
\begin{note}
\begin{el}[ul]
@ Мы допускаем отказ типа Crash, т.е. докажем невозможность для
любого более широкого класса отказов
@ Фактически мы докажем, что ни один процесс не примет решения
\end{el}
\end{note}

\bf{Пререквизиты}
\begin{el}[ul]
@ Будем считать, что процессы выбирают из множества $\{0, 1\}$.
@ Будем считать, что сеть надёжна. Т. е., все сообщения будут рано или поздно доставлены исправным процессам.
\end{el}

Состояние системы
Введём понятие "Состояния системы" GS (Global State). В GS войдёт состояние процессов. 
Мы договаривались, что работа процесса состоит из набора атомарных событий $e_i^x$.
События бывают трёх типов: внутренние, отправка сообщения
$send(p_i, m)$ и получение сообщения $recieve(p_j)$.

Состояние сети можно описать как множество пересылаемых
сообщений. Вызов процессом $p_i$ функции $recieve(p_j)$ удаляет из
множества случайное сообщение от $p_i$ к $p_j$ или ничего не делает.

Валентность состояния
Определим функцию $v(GS)$, возвращающую значение консенсуса
для состояния $GS$. Функция может вернуть $\{0\}$, $\{1\}$ или $\{0, 1\}$ в случае неопределённости.
\bf{Валетностью системы} будем называть результат функции $v(GS)$.
Т.е. система может находиться в трёх состояниях: \it{0-valent} , \it{1-valent} или \it{bivalent}.

Вход алгоритма
Через $\vec{I} = \{in_0, in_1, \dots, in_n\}$ обозначим вектор начальных значений $in_i$ для процесса $p_i$.

Положим $\vec{I}_0 = \{0, 0, . . . 0\}, \vec{I}_1 = \{1, 1, \dots, 1\}$.

\begin{thm}
Существует бивалентное начальное состояние
\end{thm}
\begin{proof}
От противного. Пусть бивалентного состояния не существует.

\begin{el}[ul]
@ Ввод $\vec{I}_0$ даст 0-валентное состояние, $\vec{I}_1$ --- 1-валентное.
@ Выпишем входы $0\dots0, 10\dots0, 110\dots0, 1\dots1$, которые из $I_0$  приводят к $I_1$. Среди них есть два соседних $\vec{I}_a$ и $\vec{I}_b$ с разной валентностью 0 и 1 соответственно.
@ Очевидно, что $\vec{I}_a$ и $\vec{I}_b$ отличаются одним битом, и пусть этот бит принадлежит процессу $p_i$.

Если существует алгоритм консенсуса, то:
@@ При отказе процесса pi после ввода $\vec{I}_a$ система должна решить 0
@@ При отказе процесса pi после ввода $\vec{I}_b$ система должна решить 1
\end{el}

Но это невозможно, т.к. после отказа $p_i$ состояние системы будет
неразличимо для остальных процессов.
\end{proof}

\begin{thm}
Пусть есть бивалентное состояние системы $C$ и есть сообщение $e = (p, m)$, применимое к этой конфигурации. Пусть $D$ --- множество состояний, достижимых из $C$ без применения $e$. И пусть $H = e(D) = \{e(E) : E \in D\}$ --- множество состояний, в которые переходят состояния $D$ после применения $e$. Тогда $H$ содержит хотя бы одно бивалентное состояние.
\end{thm}
\begin{proof}
От противного. Пусть все состояния $H$ одновалентные, т.е. или 0-валентные или 1-валентные.

Пусть $E_0$ --- 0-валентная конфигурация, достижимая из C. Она существует, т.к. $C$ --- бивалентна.
Если $E_0 \in D$, то $F_0 = e(E_0)$ и $F_0 \in H$
Если $E_0 \not\in (D \cup H)$, то существует $F_0 \in (H)$, из которого $E_0$ достижимо.
$v(E_0) = v(F_0)$, т.к. мы предположили, что у $C$ нет бивалентных потомков в $(H)$.
В любом случае $F_0 \in H$ и $v(F_0) = 0$, что и требовалось доказать.

Случай 1-валентной конфигурации доказывается аналогично.
\end{proof}

Назовём конфигурации \bf{соседними}, если их отделяет доставка одного сообщения.
Несложно показать, что существуют две соседние конфигурации $C_0$ и $C_1$, такие что $D_i = \{e(C_i) \mid i \in \{0, 1\}\}$ и $D_i$ --- i-валентная.
Для определённости положим $C_1 = e'(C_0)$ и $e' = (p', m')$.
Рассмотрим два случая: $p = p'$ и $p \ne p'$.

Консенсус (задача о византийских генералах). Есть процессы и числа. Из всех предложенных чисел нужно выбрать одно. В ситуации с отказом число должно быть предложено не отказавшим процессом. 

Сортировать надо не лексикографически(контрпример 01 10). Говорят, что достаточно выписать n состояний $0\dots0, 10\dots0, 110\dots0, 1\dots1$, которые из $I_a$  приводят к $I_b$. 


\subsection{CAP-теорема}
\subsubsection{Постановка}
Рассмотрим распределённую систему, реализующую, например, базу данных, и потребуем от неё 3 свойства:
\begin{el}[ul]
@ Consistency \\
Под консистентностью будем понимать линеаризуемость. Распределённая структура данных должна быть линеаризуемой.
@ Availability \\
Доступность означает, что любой запрос, полученный исправным процессом, должен быть обработан. Ограничений на время обработки нет, но система должна переживать серьёзные отказы сети.
@ Partition tolerance \\
Устойчивость к сетевому разделению подразумевает, что произвольное число сообщений может теряться. Или процессы могут разделиться на две группы, между которыми не будет связанности.
\end{el}

\begin{thm}
В асинхронной распределённой системе невозможно имплементировать базу данных, удовлетворяющую CAP свойствам.
\end{thm}
\begin{proof}
\begin{el}[ul]
@ Пусть это не так, и в системе есть хотя бы 2 процесса.
@ Разобъём процессы на две непересекающиеся группы $\{G_1, G_2\}$.
@ Пусть $v_0$ — начальное состояние базы.
@@ Рассмотрим выполнение $\alpha_1$, в котором была единственная запись в группе $\{G_1\}$, и пусть мы записали $v_1$.
Потребуем потери сообщений между $\{G_1, G_2\}$
@@ Рассмотрим выполнение $\alpha_2$, в котором было единственное чтение в группе $\{G_2\}$, и оно могло вернуть только $v_0$
@ рассмотрим выполнение $\alpha = \alpha_1 \alpha_2$. Для процессов $G_2$ выполнение $\alpha$ неотличимо от $\alpha_2$, т. е. чтение по-прежнему возвращает $v_0$.
@ Выполнение $\alpha$ нарушает свойство атомарности.
\end{el}
\end{proof}

Terminated  .... Есть распределённая БД, надо, чтобы все узлы, реализующие её, получили значение, которое нужно записать

Есть цепочка банков, переводим с одного счёта на другой через всю цепочку.

\subsubsection{Упражнения}
Придумать системы, удовлетворяющие двум свойствам.

CP -- не реагируем на записи, всегда возвращаем начальное состояние. 
AP -- Будем держать локальную базу данных и не общаться. Каждый поток возвращает своё.
CA -- Выделяем сервер, и пусть все узлы обращаются к нему как клиенты.

\subsubsection{Критика CAP-теоремы}
CAP теорема подразумевает модель, слишком пессимизирующую реальные сети.

Лианеризуемость --- сильное свойство атомарных объектов.

\subsubsection{Новая формулировка}
Разрешим процессам иметь локальные часы, идущие с одинаковой скоростью, но несогласованные. Введём верхнюю границу на время доставки сообщения $t_{msg}$.

Такая система называется частично-синхронной.


\subsubsection{t-Connected Consistency}
Будем называть такой косистентностью ситуацию, когда выполняются два свойства:
\begin{el}[ul]
@ Выполнение, во время которого не было потери сообщений --- атомарное
@ При выполнении с потерями существует частичный порядок $P$, такой что:
\begin{el}[ol]
@ P упорядочивает все операции записи и упорядочивает все операции чтения относительно записи
@ Чтение возвращает значение последней записи относительно P или начальное значение, если нет предыдущей записи в P
@ P сохраняет порядок операций чтений и записей, обработанных каждым узлом
@ Если операции $\alpha$ и $\beta$ разделяет интервал дольше $2 t_{msg}$, в котором не было потерь, и $\alpha$ завершилась до него, а $\beta$ началась после, то операции упорядочены в P
\end{el}
\end{el}


Реализация
Есть выделенный процесс C, хранящий оригинал данных.
read на узле A обрабатывается через запрос последней версии данных у узла C. Если удалось получить ответ за $2 t_{msg}$, то запоминаем новое значение и возвращаем результат. Если нет, то возвращаем последнее известное значение.
write на узле А тоже проксируется (пересылается) на C, но в случае таймаута вернёт клиенту подтверждение, а сам узел будет периодически перепосылать запрос на запись нового значения.
write на узле C присваивает изменению номер и запоминает.
Периодически C перепосылает всем узлам свежее значение данных.

\begin{thm}
Предложенный алгоритм является t-Connected Consistent
\end{thm}
\begin{proof}
\begin{el}[ul]
@ Атомарность на промежутке без потерь \\
В ситуации отсутствия потерь точкой линеаризации можно выбрать обработку запроса узлом C. Таким образом мы зададим глобальный порядок на всех операциях за этот интервал времени.
@ Пункты 1, 2 \\
Узел C задаёт глобальный порядок на всех операциях записи, т.к. он
присваивает им номера. Операции чтения упорядочим сразу после
соответствующей записи.
@ Пункт 3 \\
Достигается обработкой запросов в порядке получения.
@ Пункт 4 \\
Рассмотрим 4 случая, в которых две операции разделял интервал $T > t_{msg}$, в котором не было потерь.
@@ write и следующий за ним read\\
после write прошло достаточно времени, чтобы C успел узнать об этой записи, присвоить ей номер, анонсировать её (или последующую запись), и узел обработавший вторую операцию X должен был успеть получить этот анонс
@@ write и следующий за ним write
после write прошло достаточно времени, чтобы C успел узнать об этой записи и присвоить ей номер.
@@ read и следующий за ним read
первый read упорядочивается после записи $\phi$. Мы знаем, что $\phi$ и второй read упорядочены, т.е. второй read не мог вернуть значения моложе записи $\phi$
@@ read и следующий за ним write
Аналогично ситуации read read второй write упорядочится позже
\end{el}
\end{proof}

\subsection{ACID}


\subsubsection{Свойства баз данных}
Базы данных (не обязательно распределённые) обычно имеют более одного клиента. Так или иначе, эти клиенты конкурируют за доступ к базе.
Клиент работает с базой данных, выполняя операции, операции группируются в транзакции. База данных должна давать клиентам гарантии, описывающие, как они будут влиять друг на друга.

Наиболее популярным из стандартов для баз данных стал ACID, определяющий 4 свойства БД.

\begin{el}[ul]
@ Atomicity --- Атомарность\\
Либо все операции транзакции когда-нибудь будут применены, либо ни одна операция транзакции не будет применена. Т.е. отказы, потеря питания или ошибки выполнения не должны на это влиять.
@@ Когда другие клиенты увидят результат транзакции?
@@ Увижу ли я результат своей транзацкии сразу?
@@ Будет ли порядок для разных клиентов одинаковым?
@ Consistency --- Согласованность\\
Гарантирует, что результатом транзакции будет корректное состояние базы данных.
@@ Под корректным состоянием тут понимается условие соблюдения правил базы данных, например, требования целостности, триггеров или каскадных откатов транзакций.
Consistency из ACID совсем не то же, что в CAP.
@ Isolation --- Изолированность\\
Гарантирует, что конкурентное выполнение транзакций приведёт базу данных в состояние, идентичное последовательному их применению.
@@ Когда записи транзакции увидят другие клиенты?
@@ Увижу ли я собственные незакоммиченные записи?
@@ Различают разные степени изоляции, т.к. это одно из наиболее тяжёлых свойств
\begin{el}[ol]
@ Serializable
@ Repeatable reads
@ Read committed
@ Read uncommitted
\end{el}
@ Durability --- Устойчивость\\
Гарантирует, что применённая транзакция останется в истории базы данных навсегда.
\end{el}

\subsection{Степени изоляции транзакций}
Уровни изоляции принято описывать через наборы возможных влияний параллельных транзакций.

\begin{tabular}{|c|c|c|c|}
\hline
& Dirty reads & Non-repeatable reads & Phantoms\\
\hline
\hline
Read Uncommitted& {\cellcolor{red}} возможно& {\cellcolor{red}} возможно    & {\cellcolor{red}} возможно \\
\hline
Read Committed  & {\cellcolor{green}} нет & {\cellcolor{red}} возможно    & {\cellcolor{red}} возможно \\
\hline
Repeatable Read & {\cellcolor{green}} нет & {\cellcolor{green}} нет	& {\cellcolor{red}} возможно \\
\hline
Serializable    & {\cellcolor{green}} нет & {\cellcolor{green}} нет & {\cellcolor{green}} нет \\
\hline
\end{tabular}

\subsubsection{Dirty reads}
Чтение может вернуть изменения, вызванные транзакцией, которая впоследствии не подтвердится.

\begin{minipage}{.5\linewidth}
\hfil Transaction 1 \hfil   \hfil Transaction 2 \hfil
\begin{flushleft}\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\footnotesize]{sql}
/* Query 1 */
SELECT age FROM users WHERE id = 1;
/* will read 20 */
\end{minted}
\end{minipage}\end{flushleft}

\begin{flushright}\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\footnotesize]{sql}
/* Query 2 */
UPDATE users SET age = 21 WHERE id = 1;
/* No commit here and ROLLBACK later*/
\end{minted}
\end{minipage}
\end{flushright}

\begin{flushleft}\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\footnotesize]{sql}
/* Query 1 */
SELECT age FROM users WHERE id = 1;
/* will read 21 */
\end{minted}
\end{minipage}\end{flushleft}
\end{minipage}

\subsection{Non-repeatable reads}
Повторное чтение может вернуть другое состояние строки.

\begin{minipage}{.5\linewidth}
\hfil Transaction 1 \hfil   \hfil Transaction 2 \hfil

\begin{flushleft}\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\footnotesize]{sql}
/* Query 1 */
SELECT * FROM users WHERE id = 1;
\end{minted}
\end{minipage}\end{flushleft}

\begin{flushright}\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\footnotesize]{sql}
/* Query 2 */
UPDATE users SET age = 21 WHERE id = 1;
COMMIT;
\end{minted}
\end{minipage}
\end{flushright}

\begin{flushleft}\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\footnotesize]{sql}
/* Query 1 */
SELECT * FROM users WHERE id = 1;
COMMIT;
\end{minted}
\end{minipage}\end{flushleft}
\end{minipage}

\subsubsection{Phantom reads}
Два идентичных запроса возвращают разный набор строк.

\begin{minipage}{.45\linewidth}
\hfil Transaction 1 \hfil   \hfil Transaction 2 \hfil
\begin{flushleft}\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\footnotesize]{sql}
/* Query 1 */
SELECT * FROM users
WHERE age BETWEEN 10 AND 30;
\end{minted}
\end{minipage}\end{flushleft}

\begin{flushright}\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\footnotesize]{sql}
/* Query 2 */
INSERT INTO users(id,name,age)
VALUES ( 3, 'Bob', 27 );
COMMIT;
\end{minted}
\end{minipage}
\end{flushright}

\begin{flushleft}\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\footnotesize]{sql}
/* Query 1 */
SELECT * FROM users
WHERE age BETWEEN 10 AND 30;
COMMIT;
\end{minted}
\end{minipage}\end{flushleft}
\end{minipage}
    
\section{CP системы}

\begin{enumerate}
\i Master/Slave --- традиционная терминология
\i Leader/Follower --- политкорректная терминология
\i Primary/Secondary --- из мира БД; на Secondary находится не очень свежая копия Primary
\end{enumerate}

\begin{el}[ol]
@ Master всегда один
@ Процессы могут падать (crash) и восстанавливаться (recovery)
@ Синхронизация значений (например, нода вернулась, и нужно синхронизировать данные на ней)
@ инкрементальные обновления (транзакции упорядочены, и их нужно применять в том порядке, в котором они пришли)
@ Идемпотентность (повторное применение одной и той же транзакции ничего не поменяет; если мы вдруг думаем, что транзакция могла не примениться, применяем её ещё раз для подстраховки, и уверены, что плохо не станет).
@ Несколько транзакций одновременно.
@ FIFO для транзакций (от мастера к фолловерам) и FIFO для клиентов (от клиента к мастеру)
\end{el}

Производительность:
\begin{el}[ul]
@ Latency --- задержка, которую имеет система.
@ Throughput --- количество операций, которые она может одновременно выполнять.
\end{el}

Например, для сетей (вспомним):
\begin{el}[ul]
@ Throughput --- количество байт, которые можно в пике прососать.
@ Latency --- задержка между отправкой и получением подтверждения.
\end{el}

У нас же, применительно к транзакциям:
\begin{el}[ul]
@ Throughput --- количество транзакций, которые можно обрабатываать одновременно.
@ Latency --- время обработки одной транзакции.
\end{el}

\subsubsection{ZAB(ZooKeeper Atomic Broadcast)}
Есть Master, и есть Follower. Как Master'у отдавать приказы Follower'ам? Через ZAB.
ZAB реплицирует тразакции.

Гарантии:
\begin{el}[ol]
@ инкрементальные (т. е. транзакции зависимы, и ZAB должен зависимостям удовлетворять)
@ инкрементальный префикс.
@ Доставка at least once (можно больше, т. к. они идемпотентные).
\end{el}

Главный процесс ZAB состоит из двух модулей. Мастера нужно периодически выбирать, и ещё в любой момент должно быть можно узнать, кто текущий мастер.
\begin{el}[ol]
@ Leader oracle (знает, есть ли master и где его найти)
@ Leader election (руководит выборами нового master'а)
\end{el}

С сетью они не общаются напрямую, а обращаются к ZAB. Он реализует им абстракцию сети, давая повышенные гарантии, и только под ним находится настоящая сеть. ZooKeeper реализует стек протоколов/технологий (как TCP).

\begin{tikzpicture}[auto, 
block/.style = {rectangle, draw, text width = 6em, align =center},
line/.style = {draw, thick, -}]
\node [draw, rectangle] (t1)  {ZAB};
\node [draw, rectangle, right=of t1] (t2) {Storage};

\draw [->] (t1.10) -- (t2.170) node[auto] {} ;
\draw [->] (t2.190) -- (t1.350) node[auto] {} ;
%\draw [->] (t1.100) ++ (0,+2) to[above] node[auto] {} (t1.100);

\draw [->] (t1.80) to[above] node[auto, right, align=left] {abcast(m) \\ abdeliver() \\ ready()} ++ (0,+2);
\draw [->] (t1.100) ++ (0,+2) to[above] node[auto] {} (t1.100);

\draw [->] (t1.260) to[below] node[auto, right, align=left] {send(m, p) \\ recv(p)} ++ (0,-2);
\draw [->] (t1.280) ++ (0,-2) to[below] node[auto] {} (t1.280);
\end{tikzpicture}

\subsubsection{Методы ZAB}
\begin{el}[ul]
@ abcast(m) --- метод --- отправка сообщения всем нодам
@ abdeliver() --- callback --- через этот метод ZAB доставляет сообщение программе
@ ready() --- callback
@ send(m, p)
@ recv(p)
\end{el}

\subsubsection{Журнал}
ZAB хранит журнал.

В журнал записываются транзакции.

Транзакция $t = \<v, z\>$ (value, индекс), где $z = \<e, c\>$ (epoch, counter). Будем писать $c = count(z), e = epoch(z)$.
Эпоха связана со сменой leader'а. counter --- просто монотонный внутри эпохи счётчик. Получается, сам index тоже монотонный, и даёт линейный порядок.

$Q$ --- кворум, множество (или количество) процессов, на которые нужно записать данные, чтобы запись считалась успешной.
Ждать только одного бессмысленно (он может упасть), ждать всех --- тоже (кто-то мог упасть, а мы не узнаем и будем его ждать).

Пусть собрали $Q_1$, крашнулись, восстановились, собрали $Q_2$. Если $Q_1 \cap Q_2 \ne \emptyset$, мы гарантированно увидим изменения. Значит, кворум, если $|Q| > \frac{n}{2}$ процессов.


ZAB на Master по сути две службы:
\begin{el}[ol]
@ Query-Agent, на него приходят записи, делает abcast. \\
По сути ему надо просто проверить предикат (если когда делаем set, указывааем версию, то надо проверить, что она совпадает). Далее он делает abcast, и все Slave когда-то получат abdeliver.
@ Data, на него приходят чтения, делает abdeliver.
\end{el}
% TODO: схемка

ZAB на Slave ещё проще: там только Data, и он делает только abdeliver.

Надо заботиться о том, чтобы abdeliver на разных репликах делал транзакции в одном и том же порядке.

Наши требования к ZAB (Consistency):
\begin{el}[ol]
@ Integrity\\
если какой-то процесс делает $abdeliver(<v,z>)$, то $\exists$ процесс, который сделал $abcast(<v,z>)$ (т. е. сообщения не берутся из воздуха).
@ Total Order\\
Пусть $\Pi ={p_1, \dots, p_n}$ --- множество процессов (считаем, что фиксировано).

\begin{note}
Пусть $\rho_1, \dots, \rho_e, \rho_{e+1}, \dots$ --- лидеры эпох $1, \dots, e, e+1, \dots$.
При этом один и тот же процесс мог быть лидером в разные эпохи: $\rho_i = p_j \land \rho_k = p_j$. Тогда говорим
$\rho_e < \rho_{e'}$, если $e<e'$.
То есть, если бы сравнивали по номерам процессов, могло быть плохо, а если сравниваем по номерам эпох, то всё хорошо.
\end{note}

Пусть $\exists p_i$, который сделал $abdeliver(v, z), abdeliver(v', z'), \<v, z\> < \<v', z'\>$. Пусть $\exists p_j$: сделал $abdeliver(\<v', z'\>) \Then p_j$ сделал $abdeliver(<v,z>)$, причём  раньше $abdeliver(<v', z'>)$.

@ Agreement\\
Если $\begin{cases} p_i\text{ сделал }abdeliver(<v,z>)\\ p_j\text{ сделал } abdeliver(<v',z'>)\end{cases}$,\\
то $\begin{sqcases} p_i\text{ сделал } abdeliver(<v',z'>)\\p_j\text{ сделал }abdeliver(<v, z>) \end{sqcases}$.
\begin{note}
Из TO следует, что если один поток получает оба сообщения, то другой поток тоже долен получить оба.
\end{note}
То есть, наборы сообщений, доставляемые разным потокам, одинаковы.

@
\begin{el}[ol]
@@ Local order. 
Пусть была эпоха e, $abcast(v, \<e,c_1\>)$ сделан перед $abcast(v', \<e, c_2\>)$.
Если $\exists p_i$, который сделал $abdeliver(v', <e, c_2>)$, то $p_i$ сделал $abdeliver(v, <e, c_1>)$.

То есть, в пределах одной эпохи $deliver$ происходят в порядке $broadcast$ от Master'а.

@@ Global Order.
Хотим сохранить свойство из Local Order при смене эпох.
\begin{note}
Попробуем определить так:
Пусть $abcast(v, \<e,c\>)$ перед $abcast(v, \<e', c'\>)$, $e \ne e'$. Если был $abdeliver(v', \<e', c'\>)$, то был $abdeliver(v, \<e, c\>)$.
Пусть у нас текущий master сделал $abcast$ и сразу умер. Новый leader сделает другой $abcast$, а предыдущий не будет виден.
\end{note}

% Стрелочка в лекциях означает предшествование, то есть из позднего в ранний.
$\exists p_i$, который сделал $\begin{cases} abdeliver(v', <e', c'>)\\ abdeliver(v, <e, c>)\end{cases}$ $\Then abdeliver(v, <e,c>)$ предшествует  $abdeliver(v', <e,' c'>)$.
\end{el}
@ Консистентность префикса\\
TODO: вставить картинку с 22:39.

Пусть $I_e$ --- префикс транзакций от начала до конца эпохи $e$, тогда $abdeliver(\<v, z\>)$ предшествует $abcast(\<v', z'\>)$, если $epoch(z) < epoch(z')$.
\end{el}

\begin{note}
У нас из курса параллелочек есть Happens Before --- транзитивное замыкание Program Order и synchronizes-with. Оно логично переносится на модель Message Passing и называется Casual Order. Почему просто не использовать его?

У нас есть отказы, для которых мы сделали эпохи и лидеров. Но эпохи ломают HB. Пусть поток был лидером и сделал $bcast$, но он никому не дошёл, была смена лидера, потом лидером был кто-то другой, а потом опять первый поток. Тот первый $bcast$ потерялся.

См. 00094.MTS 16:34

Local Order + Global Order = Primary Order $\ne$ Casual Order
\end{note}



\subsubsection{Алгоритм}

\begin{minted}{text}
Discovery <------
|           |   |
V           |   |
Sync -------    |
|               |
V               |
Broadcast -------
\end{minted}

Будем считать, что Leader Election и Leader Oracle у нас уже как-то реализованы, и к ним можно просто обращаться.

Обозначения:
\begin{el}[ul]
@ $f.p$ --- последняя эпоха, на которую согласился follower
@ $f.a$ --- последний лидер
@ $h(f)$ --- история (журнал)
@ $f.ZxID = \<e, c\>$ --- номер последней транзакции (у нас это пара, в реализации оно записывается в одно число)
@ $Q$ --- кворум
@ $I_e$ --- инкрементальный префикс e
@ $\beta_e$ --- транзакции эпохи e
\end{el}

\paragraph{Discovery}
\begin{el}[ul]
@ F.1 
@@ \msg{CEPOCH(e)} -> Leader 
@ L.2 
@@ Лидер пытается понять, что творится на кластере после предыдущего лидера.
@@ Собирает $Q$ (включая себя туда), узнаёт, какие эпохи у фолловеров.
@@ Отправляет \msg{NEWEPOCH($e'$)}, где $e' >$ номеров эпохи всех нод, у которых он узнал.
@@ Если кворум не набирается, лидер ждёт до таймаута, снимает с себя полномочия и уходит в Discovery.
@ F.3 Получают \msg{NEWEPOCH($e'$)}.
@@ $e' < e$ $\Then$ мы получили сообщение от какого-то левого процесса, который вряд ли может быть лидером. Уходим обратно в DISCOVERY.
@@ $e' = e$: 
@@@ f.p = e
@@@ Отправляет \msg{ACK-E($h_f$)}
@@@ Переходит в \msg{SYNC}
@ L.4 
@@ Собирает $Q$ для \msg{ACK-E($h_{f_i}$)}. Эти два кворума $Q$ должны быть одинаковые, то есть мы ждём во второй раз все те процессы, которые мы набрали в первый.
@@ Не набирается $\Then$ складывает с себя полномочия и уходит.
@@ Строит $I_e = \{z \in h_{f_I} \mid z' \le max(z(h_{f_i}))\}$ (то есть выбирает фолловера с самой длинной историей и начинает отсчёт с неё (берёт всё, что было раньше её конца)).
@@ Переходит в \msg{SYNC}
\end{el}
% TODO: пример с картинкой

\paragraph{SYNC}
\begin{el}[ul]
@ L.1 
@@ Отправляет \msg{NEWLEADER($e', I_{e'}$)} в $Q$.
@ F.2 
@@ atomic\{\\
$f.a = \rho_{e'}$\\
$h_f = I_{e'}$ --- отправляет просто весь журнал, в реальности можно отправлять только отсутствующую часть.\\
\}
@@ Отправляет \msg{ACK-LD} --- согласие на лидерство L.
@ L.3 
@@ Собирает кворум $Q$ из тех, кто прислал \msg{ACK-LD}.
@@ Отправляет \msg{COMMIT} в $Q$.
@ F.4
@@ Получает \msg{COMMIT}.
@@ Делает $abdeliver(\<v, z\>)$ в порядке $z$ для $\<v, z\>$, для которых ещё не было $abdeliver$
@@ В этот момент лидер становится полноценным лидером и начинает принимать и обрабатывать запросы.
\end{el}

\paragraph{BROADCAST}
\begin{el}[ul]
@ L.1 
@@ Тот процесс $p_i$, который считает себя лидером, делает $ready(e')$ и сообщает Query-Agent, что он уже может давать новые пары $\<v, z\>$.

@ F.2
@@ \msg{COMMIT($v, z$)}

@@ делает $abdeliver(v', z')$ для таких $z$, что $z' \le z$ и не было $abdeliver(v, z')$

@ L.3
@@ Отправляет $abcast(\<v, z\>)$
@@ Делает $abdeliver(z')$ для $z'$, т. ч. $z' \le z$ и не было $abdeliver(z')$.

@ L.4 
@@ Если какой-то процесс $p_j$ ожил и захотел вернуться:
@@@ Лидер получает от $p_j$ \msg{CEPOCH()}
@@@ Отправляет ему \msg{NEWEPOCH($e'$)} и \msg{NEWLEADER($I_e + \beta_{e'}$)}.
@@@ Когда получит от $p_j$ \msg{ACK-LD}, добавит его в кворум: $Q = Q \cap \{p_j\}$

@@ Если какой-то процесс $p_j$ умер:
@@@ Удаляем его из кворума.
@@@ Если кворум не набирается, складываем с себя полномочия и уходим в DISCOVERY.
\end{el}

\subsubsection{Детали реализации}
Иногда выделяют выборы лидера (Leader Election) в фазу 0.
Чтобы снизить объём передаваемых по сети данных, можно всегда выбирать Leader с самой длинной историей, чтобы не отправлять это всё по сети.

Можем потерять что-то в журнале: 
Пусть лидер был выбран, получил какие-то данные, но упал, не успев их отправить. Был выбран новый лидер, а потом старый вернулся, попытался подключиться, и он должен выкинуть те записи, которые есть только у него.

Вместо \msg{NEWLEADER} отправляется три разных вида сообщений:
SNAP --- узел очень отстал, поэтому не качать журнал, а качать snapshot (периодически делаются слепки состояния) и докачать отсутствующие в нём транзакции.

\subsubsection{Верны ли свойства?}
\begin{el}[ul]
@ Integrity --- сообщениям просто неоткуда браться из воздуха.
@ TO --- вроде правда.
@ abdeliver делаем только после того, как сообщение будет записано на кворум Follower'ов. Потерять можем только если не было $abdeliver$.
@Agreement --- вроде тоже.
@ Local Order. Когда происходит COMMIT, доставляются все транзакции, не доставленные до той, о которой пришло подтверждение.
@ Global Order. Верно.
@ Консистентность префикса.
В конце SYNC делаем COMMIT и только потом ready. Это и гарантирует. 
\end{el}

\section{AP системы}
Есть пользователь, который хочет купить что-то в интернет-магазине. Его запрос о добавлении товара в корзину прилетел на какой-то фронтенд в каком-то дата-центре. Представим, что все наши дата-центры в огне, мы не смогли обработать запрос и вернули ошибку пользователю. Пользователь ушёл, маркетологи прибежали и настучали за недоступность сервиса. 

Да, по CAP-теореме все запросы обработать не можем. Но маркетологи-то злые и не понимают. Что делать?

Какие требования есть к корзине?
\begin{enumerate}
\i Добавил в корзину --- не значит купил. Выдавать ошибку не надо, просто надо сделать хоть что-то. Запрос где-то сохраняется, обработаем позже.
\i Корзина неконсистентна. Добавляем товары в корзину на разных вкладках, в разное время, и она может быть в разных состояниях.
\i Корзина всегда доступна на запись. Главное --- записать, синхронизируем потом.
\end{enumerate}
Это похоже на AP-систему.

Что хотим от системы?
\begin{enumerate}
\i Key-Value хранилище (реляционная БД не всегда нужна, для реализации корзины вполне достаточно key-value, где пользователь --- ключ).
\i Шардирование (размазывать данные по узлам, не хранить всё в одном месте) \& репликация.
\i Всегда доступны на запись.
\i Версионирование values (чтобы понимать, какой из наборов данных настоящий).
\i Децентрализована.
\i Масштабирование без деградаций.
\end{enumerate}

Люди из Amazon написали это и назвали Amazon DynamoDB.

\subsubsection{Наши хотелки в формальной форме}
\begin{enumerate}
\i Формальное описание базы данных (Query language)
key $\to$ value, один запрос --- один ключ
\i ACID 
\begin{enumerate}
\i Atomicity --- атомарность всегда будет, ибо в транзакции только одна операция, и не о чем говорить
\i Consistent --- не будет (если только очень слабая)
\i Isolated --- нет
\i Durability --- eventually (сколько сможем: если ноды упали, но вернулись, сможем получить с них данные)
\end{enumerate}
\i Efficiency --- 99.9\% (перцентиль --- распределение времён ответов, и смотрим на статистику для перцентили 99.9\%)
\i Кастомизация --- дать пользователю настраивать базу данных, tradeoff времени ответа и степени консистентности
\i AAA --- Authentication, Authorization, Accounting --- нет. Каждый сервис поднимает свою инсталляцию.
\i Symmetry --- не должно быть выделенных узлов, играющих особую роль, поскольку их отказы будут критичны (т. е. хотим, чтобы узлы имели одинаковую функциональность).
При записи узлы пишут с разной скоростью. Пусть они общаются между собой, делая вместе асинхронно лучше и быстрее (аналог P2P-сетей).
\i Гетерогенность --- учитывать то, что у нас разное оборудование разной производительности.
\end{enumerate}

\subsection{Проблемы и решения}

\begin{tabular}{|l|l|l|}
\hline
Проблема & Техника & Профит \\ \hline
Шардирование & Консистентные хэши & Инкрементальный рост \\ \hline
HA (Highly Available) на запись & Векторные часы и reconcilation на чтение & HA и Durability\\ \hline
Обработка отказов & Sloppy Quorums + Hinted Handoffs & Решили проблему \\ \hline
Recovery & Anti-entropy и Merkle Trees & Восстановление ... \\ \hline
Membership и Failure detection & gossip negotiation & Решили проблему \\ \hline
\end{tabular}

\subsubsection{Шардирование}
Строим БД.

Как у нас устроены key-value хранилища? Хэш-таблица или дерево? Хэши, конечно. Как вообще распределять и параллелить деревья?

Пользователь имеет id в диапазоне $[0, R]$. Пусть есть сервера $S_1, \dots, S_N$. Каждому серверу сопоставляем случайное число $T_1, \dots, T_N$.
Данные из диапазона $[T_i, T_{i+1}]$ будем хранить на сервере $S_i$.

Пусть приходит новый сервер $S_{N+1}$ с числом $T_{n+1}$. Вспомним обычную хэш-таблицу. Перехэширование полностью переписывало все данные. Мы не можем себе это позволить. Поэтому смотрим, куда попадает $T_{N+1}$, забираем часть данных от текущей дуги и отправляем на новый.

Итого, переезжает лишь $[\frac{R}{N}]$ данных.
Это называется "консистентный хэш".

Проблемы: 
\begin{enumerate}
\i Неравномерность распределения данных
\i Неравномерность использования оборудования
\end{enumerate}

Решение: серверу сопоставляем не ключ, а последовательность ключей $T_{i, 1}, \dots, T_{i, m_i}$. Количество таких кусков $m_i$ зависит от производительности сервера. Этот метод можно рассматривать как "виртуальные сервера". Итак, мы размазали данные более равномерно и учли производительность.

Репликация:
$D$ --- число копий.

В нашей системе без репликации $R_0 \in T_1$.
Сделаем $R_0 \in T_1, \dots, T_D$.

При добавлении нового сервера нужно поддерживать это. То есть, на новый сервер записать все дубликаты, а с какого-то стереть.

Проблема:
\begin{el}[ul]
@ Учитываем виртуальность. \\
Реальных репликаций может оказаться меньше. Токены принадлежат виртуальным серверам, а они могут быть на одном физическом.

@ Учитываем физические условия.
Если у нас есть два дата-центра, хочется хранить реплики в разных дата-центрах, чтобы не было failed domain --- когда отвалился дата-центр, стойка, коммутатор, что-то ещё. 
\end{el}

Надо разметить все наши сервера по этим признакам и выбирать места для дубликатов с учётом этого.

\subsubsection{HA на запись}

Durability --- если прошла запись, то она сохранится, не потеряется.

API: $put(key, value, ctx)$
$get(key) -> [(v_1, ctx_1), \dots, (v_n, ctx_n)]$

У нас может быть неконсистентность, поэтому иногда БД не сможет различить, что надо вернуть. Поэтому она возвращает несколько вариантов, в надежде на то, что пользовательская программа сможет найти нужное ей значение по контексту ctx.

Можно сделать ctx=версия (версионирование).
Но проблема в линейности этого версионирования.
Пусть прилетели 2 запроса $put(k, v_1, x)$ и $put(k, v_2, y)$ на два узла, между которыми не было соединения на момент обработки запроса. Когда связность вернётся, придётся решить, какая из записей настоящая.

Заставим решать проблему БД.
Если выкинуть какой-то ключ (случайно или по записям системных часов), нарушаем Durability. Но есть случаи, когда БД сможет различить, тогда хорошо, знаем, что вернуть.
Если не можем решить на уровне БД, заставляем решать пользовательское приложение.

Векторные часы (воспоминание из курса параллелочек).
\begin{enumerate}
\i int
Пусть есть $p_1$ с часами $c_1$ и $p_2$ с часами $c_2$. При получении сообщения с меткой времени $ctx$ процессом его часы $c_i = max(ctx, c_2)+1$. 
Проблема в том, что много информации теряется.

\i vector
Каждый процесс хранит часы каждого из других процессов в виде вектора, и ctx=этот вектор. Происходит событие --- увеличиваем свои часы. При получении сообщения берём максимум из позиций, которые не свои, а свою увеличиваем на 1 (так обеспечиваем транзитивность).

Могут быть проблемы, если вектора становятся длинными (чтоы понять, кто более новый, надо сортировать вектора, это за $N \log N$). Это решается ...

\i matrix
Тоже есть, вспоминаем сами.
\end{enumerate}

Как пользовательская программа может разрешать неоднозначности?
Если в корзину можно только добавлять, то можно просто объединить списки продуктов и сказать потом базе, что на самом деле пользователь добавил всё вместе (cделать put нового значения). 
База сама поймёт по векторным часам, что то значение новое, а предыдущие несколько --- старые.
Это и есть reconcilation --- обновление значений.

При этом при любом get база может смотреть, у кого старые значения, и говорить им, что надо обновить, чтобы поддерживать актуальное состояние.

\subsubsection{Обработка отказов}
% N на самом деле D. Но это не точно.
Если $Q>\frac{N}{2}$, пересечение кворумов будет непусто, и потери данных не произойдёт.
Будем собирать отдельные кворумы на чтение и на запись. Если $|Q_R| + |Q_W|>N$, последнюю запись всегда будет видно.

Sloppy Quorums означает, что N не фиксировано. Если сломались все ноды $S_1, \dots, S_D$, пишем на $S_{D+1}$ и на другие, до которых дотянемся. Но при этом если сразу же после такой записи сделаем чтение, записи можем не увидеть: если ноды поднялись, то будем читать с них, а на них нет информации (ещё не синхронизировались). То есть, можем получить старое значение, новое или оба сразу.

Владельцы чужих записей выставляют на не свои записи флаг foreign и периодически сообщают $T_1, \dots, T_D$ (истинным хозяевам), что нужно их забрать (такое обращение называется hint). Это и есть Handoff (вспомним помогание в lock-free алгоритмах).

Считая, что пользователь не слишком быстрый, и не будет непрерывного потока операций с корзиной, это работает неплохо.

\subsubsection{Recovery}
Узел пропал и вернулся $\Then$ может восстановиться. 

Анти-энтропия --- сравнение всех реплик одного и того же блока и обновление их до актуальной версии. Допустим, упавший узел восстановился, на нём есть какие-то реплики, и надо проверить, актуальны ли они.
 
Пусть есть $T_1, T_2$ с общим Range $R_0$. В какой-то момент $T_2$ решает, что надо синхронизироваться. Обмениваться всеми данными по сети долго (может быть, они даже согласованы, тогда ничего делать не надо).

Merkle Trees --- деревья хэшей. То есть от куска считается хэш, он делится пополам, от половинок тоже считается хэш, и так далее.

Обмениваемся хэшами корня дерева, если совпали, ОК, если нет, обмениваемся хэшами на уровень ниже. Так можно вполне быстро понять, что именно поменялось.
Это допускает асинхронную синхронизацию.

Это используется в Bitcoin, Ethereum, magnet-ссылках в торрентах.

\subsubsection{Membership и Failure detection}
Пусть есть $T_1, T_2, T_3$. Есть клиент $C_1$ и range ключей $R_0$. Тогда всегда $C_0$ обращается к $T_1$, а он потом распределяет на $T_2, T_3$. Если $T_1$ недоступен, все идут в $T_2$. Это гарантирует нам б\'{о}льшую упорядоченность, и у нас меньше мороки с синхронизацией.

Проблема: может возникнуть bottleneck. Пусть пришла куча клиентов на $T_1$. Он может стать перегружен.  Мы теряем симметричность нод, у них появляются особые роли.
Поэтому пусть клиент обращается к любому из $T_1, T_2, T_3$. Но клиент ничего не знает о range и внутреннем устройстве БД и не может выяснить, где $T_1, T_2, T_3$. Так пусть он пойдёт к любому узлу, а они уже сами распределят между собой. При этом синхронно выяснять кто есть кто мы не можем, ибо нода могла вообще партицироваться в тот момент и ни до кого не достучаться.

Как добавлять новые ноды? Опять же, синхронно это делать плохо. 

Решение: добавлять и удалять ноды будет оператор. Новая нода будет просто рассказывать всем о своём существовании, а писать на неё данные, пока она не появится в топологии, никто не будет, и запросы на неё приходить тоже не будут. Если какой-то узел упал, мы думаем, что он поднимется, и не начинаем реплицировать данные, которые на нём были, на какой-то ещё узел.

Мы будем хранить версионированную топологию сети, которую будет менять оператор. Пусть оператор пришёл и сказал о новой топологии сети. Любому узлу. А узлы между собой должны распределить новую информацию.
Узлы будут асинхронно рассказывать друг другу о новой топологии. В итоге все узнают о новости примерно за логарифм.

Можно ускорить.
Возьмём несколько нод, пометим звёздочками. Будем говорить, что это узлы-информаторы, которые знают самые последние новости, и о новой топологии надо спрашивать сначала их. Потеряли в симметрии, но это внутренний контрольный трафик, он небольшой, и такая потеря в симметрии не страшна.
Эти ноды называются seed nodes. 

\subsection{Как быстро мы работаем?}
Для записи клиент ищет любую ноду, та ищет $|C_W|$ нод и пишет на них.
$O(ping+flush*D)$

Чтение --- то же самое, только собираем кворум на чтение и не тратим время на запись.
$O(ping)$

Кастомизация: меняем значения. Обычно
$|C_W|$ = 2-3, $|C_W|$ = 2-3, $D$ = 3-4.

\subsection{Консистентное хэширование}
\begin{el}[ul]
@ Вариант S1 --- см. выше.
Проблемы: неодинаковые куски, неоднородные хэши (получаются "горячие участки").

@ Вариант S2. 
Делаем кольцо, разбиваем на range-и $R_1, \dots, R_n$ равномерно. Сервер генерирует себе токены $T_1, \dots, T_{m_i}$. Токен попадает в какой-то range $\Then$ сервер берёт себе этот range. Если какие-то куски остались непокрытыми, надо было сгенерировать больше токенов.
Промежуточная версия для перехода от S1 к S3, может работать даже медленнее S1.

@ Вариант S3. Узел выбирает себе range'и исходя из загруженности этих range.
\end{el}

\end{document}
