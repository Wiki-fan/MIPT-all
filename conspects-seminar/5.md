Обращение к дискам может стать узким местом в вычислениях. Обычный API для дисков плох, ибо он не параллелен, и ибо нагружает железо.
Можно доверить это всё MPI, которая умеет параллельно читать, разбивать файлы по разным дискам, кэшировать и всё такое. Это достигается за счёт того, что оно собирает записи в пачки и пишет пачками.

write_all говорит, что это массовая запись, он узнаёт, что так будут делать все потоки, и соптимизирует

Чтобы работать с глобальным дескриптором вызываем shared. Гарантируется, что одновременно идёт только одна запись. Порядок  не гарантируется. 
Если хотим порядок, пишем ordered. Порядок выстраивается в соответствии с рангами процесса внутри коммуникатора.

struct {
int x[2];
int y[2];
int z[2];
};

Хотим вывести в файл с каждого процесса.
x_0^0, x_1^0, y_0^1, y_1^1, z_0^2, z_1^2, x_1^0, x_

MPI_File_set_view(olsp, etype, dtype, ...) 

0: view(0, int, ++0000)
1: view(0, int, 00++00)
2: view(0, int, 0000++)

Ну либо можно
0: view(0, int, ++0000)
1: view(2*(sizeof(y)), int, ++0000)
2: view(2*(sizeof(y)+sizeof(x)), int, ++0000)

Есть матрица, хранящяяся распределённо в блочном виде:
 0  1| 4  5
 2  3| 6  7
-----+-----
 8  9|12 13
10 11|14 15

Хотим записать её в файл именно в таком виде.

native -- платформозависим
internal -- внутреннее представление MPI, зависим от версии MPI
external32 -- описано в стандарте, читается везде

mpi_type_darray всё это тоже умеет